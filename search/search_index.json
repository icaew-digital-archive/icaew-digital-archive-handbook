{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"ICAEW Digital Archive Handbook","text":"<p>Last Updated: November 26, 2024</p>"},{"location":"index.html#introduction","title":"Introduction","text":"<p>Purpose: The ICAEW Digital Archive Handbook serves as the central resource for all digital preservation and web archiving activities at ICAEW. This comprehensive guide provides detailed information about our digital preservation platforms, processes, and best practices.</p>"},{"location":"index.html#contact-information","title":"Contact Information","text":"<p>For immediate assistance or questions:</p> <ul> <li>Digital Archive Manager: Craig McCarthy</li> <li>Digital Archivist: Nick Kelly</li> </ul>"},{"location":"useful-resources.html","title":"Useful Resources","text":""},{"location":"useful-resources.html#overview","title":"Overview","text":"<p>Purpose: This section provides a comprehensive collection of resources related to digital preservation and web archiving at ICAEW. These resources are organized by platform and category for easy reference.</p>"},{"location":"useful-resources.html#platform-specific-resources","title":"Platform-Specific Resources","text":""},{"location":"useful-resources.html#preservica","title":"Preservica","text":"Resource Description URL Universal Access Primary access point for users icaew.access.preservica.com WordPress Admin WordPress administration interface icaew.access.preservica.com/wp-login.php Admin Interface Administrative interface eu.preservica.com Service Desk Technical support and issue resolution preservica.freshdesk.com Community Hub User community and knowledge base community.preservica.com User Group Legacy community forum usergroup.preservica.com"},{"location":"useful-resources.html#archive-it","title":"Archive-It","text":"Resource Description URL Public Collections Primary access point for users archive-it.org/organizations/1613 Admin Interface Administrative interface partner.archive-it.org Help Centre and Training Training resources and documentation support.archive-it.org"},{"location":"useful-resources.html#icaew-systems","title":"ICAEW Systems","text":"Resource Description URL Sitecore ICAEW.com content management system master.icaew.com/sitecore/login Semaphore SmartLogic Taxonomy management system cloud.smartlogic.com/spa/ Smartlogic Portal Additional taxonomy resources portal.smartlogic.com"},{"location":"useful-resources.html#web-archiving-tools-and-documentation","title":"Web Archiving Tools and Documentation","text":"Tool/Resource Description URL Browsertrix-crawler Browser-based high-fidelity web crawler github.com/webrecorder/browsertrix-crawler Browsertrix Documentation Official documentation and guides crawler.docs.browsertrix.com Wget Manual GNU Wget documentation www.gnu.org/software/wget/manual/ WARC Tools Tools for working with WARC files github.com/internetarchive/warc Web Archive Validator Validation tools for web archives github.com/iipc/webarchive-validator Wayback Machine Internet Archive's web archive web.archive.org"},{"location":"useful-resources.html#digital-preservation-standards-and-guidelines","title":"Digital Preservation Standards and Guidelines","text":"Resource Description URL OAIS Reference Model ISO 14721:2012 Open Archival Information System iso.org/standard/57284.html PREMIS Data Dictionary Preservation metadata standard loc.gov/standards/premis/ Dublin Core Metadata Initiative Metadata element set standard dublincore.org NDSA Levels of Preservation Digital preservation maturity model ndsa.org/activities/levels-of-digital-preservation/ Digital Preservation Coalition UK-based digital preservation community dpconline.org IIPC Guidelines Web archiving best practices netpreserve.org/web-archiving/"},{"location":"useful-resources.html#file-format-resources","title":"File Format Resources","text":"Resource Description URL PRONOM File format registry nationalarchives.gov.uk/PRONOM Library of Congress Format Descriptions File format documentation loc.gov/preservation/digital/formats/ Wikidata File Format Items File format information wikidata.org/wiki/Q235557"},{"location":"useful-resources.html#tooling-documentation","title":"Tooling Documentation","text":"Tool Description URL ExifTool Comprehensive metadata management tool exiftool.org ExifTool Documentation Complete documentation and examples exiftool.org/exiftool_pod.html Brunnhilde File analysis and virus checking tool github.com/tw4l/brunnhilde Siegfried File format identification tool itforarchivists.com/siegfried MediaInfo Media file metadata extraction mediaarea.net/MediaInfo"},{"location":"useful-resources.html#training-and-educational-resources","title":"Training and Educational Resources","text":""},{"location":"useful-resources.html#books-and-publications","title":"Books and Publications","text":"Title Availability The Theory and Craft of Digital Preservation Available from the ICAEW library Digital Preservation for Libraries, Archives, and Museums Available from the ICAEW library Web Archiving (by Julien Masan\u00e8s) Available from the ICAEW library"},{"location":"useful-resources.html#webinars-and-training","title":"Webinars and Training","text":"Resource Description URL Digital Archiving - What Is It! TfL Corporate Archives Webinar youtube.com/watch?v=2NgzW5YqUXA DPC Webinars Digital Preservation Coalition training sessions dpconline.org/events/ IIPC Training International Internet Preservation Consortium training netpreserve.org/training/"},{"location":"useful-resources.html#community-and-professional-organizations","title":"Community and Professional Organizations","text":"Organization Description URL Digital Preservation Coalition UK-based digital preservation community dpconline.org International Internet Preservation Consortium Web archiving community netpreserve.org Society of American Archivists Professional archival organization archivists.org Archives and Records Association UK and Ireland professional body archives.org.uk"},{"location":"useful-resources.html#government-and-institutional-resources","title":"Government and Institutional Resources","text":"Resource Description URL National Archives Digital Preservation UK National Archives guidance nationalarchives.gov.uk/information-management/manage-information/preserving-digital-records/ Library of Congress Digital Preservation US Library of Congress resources loc.gov/preservation/ UK Web Archive British Library web archiving initiative webarchive.org.uk UK Government Web Archive National Archives web archive webarchive.nationalarchives.gov.uk"},{"location":"admin-guides/digital-archive-workstation.html","title":"Digital Archive Workstation","text":""},{"location":"admin-guides/digital-archive-workstation.html#overview","title":"Overview","text":"<p>Purpose: This document provides comprehensive information about the digital archiving workstation setup, including system configuration, installed software, and maintenance procedures.</p> <p>The workstation runs Ubuntu/BitCurator as the primary operating system.</p>"},{"location":"admin-guides/digital-archive-workstation.html#installation-guide","title":"Installation Guide","text":""},{"location":"admin-guides/digital-archive-workstation.html#installing-ubuntubitcurator","title":"Installing Ubuntu/BitCurator","text":"<p>Follow these steps to set up a new laptop:</p> <ol> <li>Download Ubuntu </li> <li> <p>Obtain the Ubuntu image from ubuntu.com</p> </li> <li> <p>Create Bootable USB </p> </li> <li>Use Ubuntu's \"Startup Disk Creator\" or balenaEtcher</li> <li> <p>Follow the tool's instructions to create a bootable USB</p> </li> <li> <p>Install Ubuntu </p> </li> <li>Restart and press <code>F12</code> (or appropriate boot menu key) to boot from USB</li> <li>Select \"Erase disk and install Ubuntu\" or \"Install Ubuntu\" depending on your needs</li> <li>Follow the installation wizard</li> <li> <p>Default naming conventions:</p> <ul> <li>Username: <code>digital-archivist</code></li> <li>Computer name: <code>digital-archive</code></li> </ul> </li> <li> <p>Install BitCurator </p> </li> <li>Use the SaltStack repository</li> <li> <p>Follow the installation instructions provided</p> </li> <li> <p>Post-Installation <pre><code>sudo apt update &amp;&amp; sudo apt upgrade\n</code></pre></p> </li> </ol>"},{"location":"admin-guides/digital-archive-workstation.html#software-installation","title":"Software Installation","text":""},{"location":"admin-guides/digital-archive-workstation.html#pre-installed-software","title":"Pre-Installed Software","text":"<p>The following tools come pre-installed with BitCurator:</p> <ul> <li>Ubuntu - Core operating system</li> <li>BitCurator - Digital archiving suite</li> <li>Python - Programming language</li> <li>Git - Version control</li> <li>Docker - Container platform</li> </ul>"},{"location":"admin-guides/digital-archive-workstation.html#additional-software-to-install","title":"Additional Software to Install","text":""},{"location":"admin-guides/digital-archive-workstation.html#development-tools","title":"Development Tools","text":"<p>Python Virtual Environment Support: <pre><code>sudo apt install python3.10-venv\n</code></pre></p> <p>Git Configuration: <pre><code>git config --global user.name \"FIRST_NAME LAST_NAME\"\ngit config --global user.email \"MY_NAME@example.com\"\n</code></pre></p> <p>Visual Studio Code: Install via Ubuntu Software app, or via command line: <pre><code>wget -qO- https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor &gt; packages.microsoft.gpg\nsudo install -o root -g root -m 644 packages.microsoft.gpg /etc/apt/trusted.gpg.d/\nsudo sh -c 'echo \"deb [arch=amd64,arm64,armhf signed-by=/etc/apt/trusted.gpg.d/packages.microsoft.gpg] https://packages.microsoft.com/repos/code stable main\" &gt; /etc/apt/sources.list.d/vscode.list'\nsudo apt update\nsudo apt install code\n</code></pre></p> <p>Recommended extensions: - Python - Prettier - Markdown All in One</p>"},{"location":"admin-guides/digital-archive-workstation.html#web-archiving-tools","title":"Web Archiving Tools","text":"<p>Google Chrome/Chromium: Required for browsertrix-crawler <pre><code># Install Google Chrome\nwget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\nsudo apt install ./google-chrome-stable_current_amd64.deb\n\n# Or install Chromium\nsudo apt install chromium-browser\n</code></pre></p> <p>AWS CLI: For large file ingestion into Preservica <pre><code>curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install\n</code></pre></p> <p>Get cookies.txt: Chrome extension for cookie export - Install from Chrome Web Store</p> <p>Thunar: Bulk file rename utility <pre><code>sudo apt install thunar\n</code></pre></p>"},{"location":"admin-guides/digital-archive-workstation.html#digital-archiving-tools","title":"Digital Archiving Tools","text":"<p>archiveweb.page: Chrome extension for web archiving - Install from Chrome Web Store</p> <p>replayweb.page: Web archive replay tool - Available online</p> <p>OCRmyPDF: PDF OCR tool <pre><code>sudo apt install ocrmypdf\n</code></pre></p> <p>ExifTool: Metadata management tool <pre><code>sudo apt install libimage-exiftool-perl\n</code></pre></p> <p>yt-dlp: Enhanced fork of youtube-dl with additional features <pre><code>pip install yt-dlp\n</code></pre></p> <p>py-wacz: WACZ format handler <pre><code>pip install py-wacz\n</code></pre></p> <p>py-wasapi-client: WARC file download client <pre><code>pip install py-wasapi-client\n</code></pre></p>"},{"location":"admin-guides/digital-archive-workstation.html#documentation-tools","title":"Documentation Tools","text":"<p>MkDocs: Documentation generator <pre><code>pip install mkdocs\n</code></pre></p> <p>Material: MkDocs theme <pre><code>pip install mkdocs-material\n</code></pre></p>"},{"location":"admin-guides/digital-archive-workstation.html#vdi-access","title":"VDI Access","text":"<p>Citrix Workspace: Version 23.3.0.32 (tested working) for VDI access via: - MKPortal - LonPortal</p>"},{"location":"admin-guides/digital-archive-workstation.html#additional-resources","title":"Additional Resources","text":"<ul> <li>BitCurator Documentation</li> <li>Ubuntu Documentation</li> <li>Docker Documentation</li> <li>Python Documentation</li> <li>Git Documentation</li> </ul>"},{"location":"admin-guides/editing-digital-archive-handbook.html","title":"Editing the ICAEW Digital Archive Handbook","text":""},{"location":"admin-guides/editing-digital-archive-handbook.html#overview","title":"Overview","text":"<p>Purpose: This guide provides comprehensive instructions for editing and maintaining the ICAEW Digital Archive Handbook, which serves as the central resource for digital preservation and web archiving activities at ICAEW.</p>"},{"location":"admin-guides/editing-digital-archive-handbook.html#tools-and-technologies","title":"Tools and Technologies","text":""},{"location":"admin-guides/editing-digital-archive-handbook.html#core-documentation-tools","title":"Core Documentation Tools","text":"<ul> <li> <p>MkDocs A static site generator that's geared towards building project documentation</p> </li> <li> <p>Material Material theme for MkDocs</p> </li> </ul>"},{"location":"admin-guides/editing-digital-archive-handbook.html#development-environment-setup","title":"Development Environment Setup","text":""},{"location":"admin-guides/editing-digital-archive-handbook.html#python-environment","title":"Python Environment","text":"<p>Tip: MkDocs and Material require a Python installation. Good practice is to initialize a Python virtual environment before installing the libraries:</p> <pre><code>python3 -m venv venv\n</code></pre> <p>Activate the virtual environment: <pre><code>. ./venv/bin/activate\n</code></pre></p>"},{"location":"admin-guides/editing-digital-archive-handbook.html#package-installation","title":"Package Installation","text":"<p>Install MkDocs: <pre><code>pip install mkdocs\n</code></pre></p> <p>Install Material theme: <pre><code>pip install mkdocs-material\n</code></pre></p>"},{"location":"admin-guides/editing-digital-archive-handbook.html#additional-tools","title":"Additional Tools","text":"<ul> <li>Visual Studio Code   Recommended code editor with Markdown support</li> <li> <p>Install the following extensions:</p> <ul> <li>Markdown All in One</li> <li>Markdown Preview Enhanced</li> <li>Python</li> <li>Prettier</li> </ul> </li> <li> <p>Git   Version control system for tracking changes</p> </li> <li> <p>Python Virtual Environment   For managing Python dependencies</p> </li> </ul>"},{"location":"admin-guides/editing-digital-archive-handbook.html#development-workflow","title":"Development Workflow","text":""},{"location":"admin-guides/editing-digital-archive-handbook.html#github-workflow","title":"GitHub Workflow","text":"<p>When contributing to the handbook, follow these steps to manage your changes:</p> <ol> <li> <p>Create a branch from <code>master</code>: <pre><code>git checkout -b craig\n</code></pre></p> </li> <li> <p>Make your changes (edit, stage, commit):</p> <p>Note: This section will usually be done in the GUI of VSCode rather than via these commands.</p> <pre><code>git add .\ngit commit -m \"Add awesome feature\"\n</code></pre> </li> <li> <p>Push your branch to GitHub: <pre><code>git push -u origin craig\n</code></pre></p> </li> <li> <p>Keep your branch up to date with <code>master</code>: <pre><code>git fetch origin &amp;&amp; git merge origin/master &amp;&amp; git push\n</code></pre></p> </li> </ol>"},{"location":"admin-guides/editing-digital-archive-handbook.html#local-development","title":"Local Development","text":"<p>To edit and preview changes in real-time: <pre><code>mkdocs serve\n</code></pre></p> <p>This starts a local development server (typically at <code>http://127.0.0.1:8000</code>) that automatically reloads when you save changes to your Markdown files.</p>"},{"location":"admin-guides/editing-digital-archive-handbook.html#building-static-site","title":"Building Static Site","text":"<p>To create a static site for offline viewing (e.g., in the ICAEW VDI): <pre><code>mkdocs build\n</code></pre></p> <p>This creates a static website in the <code>/site/</code> directory. Note: The <code>/site/</code> directory is excluded from Git (see <code>.gitignore</code>) as it's generated content.</p>"},{"location":"admin-guides/editing-digital-archive-handbook.html#deployment","title":"Deployment","text":"<p>The contents of <code>/site/</code> should be copied to: <pre><code>G:\\Apps\\Passport\\ICAEW Digital Archive Handbook\n</code></pre></p> <p>Deployment steps: 1. Build the static site: <code>mkdocs build</code> 2. Copy the entire contents of the <code>/site/</code> directory to the deployment location 3. Ensure all files are copied, including assets, JavaScript, and CSS files</p>"},{"location":"admin-guides/editing-digital-archive-handbook.html#documentation-structure","title":"Documentation Structure","text":""},{"location":"admin-guides/editing-digital-archive-handbook.html#file-organization","title":"File Organization","text":"<ul> <li>Main documentation files are located in the <code>/docs/</code> directory</li> <li>Configuration is managed in <code>mkdocs.yml</code></li> <li>Custom CSS can be added in <code>/docs/assets/css/custom.css</code></li> <li>Images should be placed in <code>/docs/assets/images/</code></li> <li>The generated site is output to <code>/site/</code> (excluded from Git)</li> </ul>"},{"location":"admin-guides/editing-digital-archive-handbook.html#navigation","title":"Navigation","text":"<p>The navigation structure is defined in <code>mkdocs.yml</code> under the <code>nav:</code> section. Key sections include:</p> <ul> <li>User Guides</li> <li>Admin Guides</li> <li>Useful Resources</li> <li>Logins</li> </ul> <p>To add a new page: 1. Create the Markdown file in the appropriate directory under <code>/docs/</code> 2. Add the entry to the <code>nav:</code> section in <code>mkdocs.yml</code> 3. Follow the existing indentation and formatting pattern</p>"},{"location":"admin-guides/editing-digital-archive-handbook.html#adding-images","title":"Adding Images","text":"<ol> <li>Place image files in <code>/docs/assets/images/</code></li> <li>Reference them in Markdown using:    <pre><code>![Alt text](assets/images/filename.png)\n</code></pre></li> <li>Supported formats: PNG, JPG, JPEG, GIF, SVG</li> </ol>"},{"location":"admin-guides/editing-digital-archive-handbook.html#best-practices","title":"Best Practices","text":""},{"location":"admin-guides/editing-digital-archive-handbook.html#markdown-guidelines","title":"Markdown Guidelines","text":"<ul> <li>Use clear, descriptive headings</li> <li>Keep line length reasonable (80-100 characters)</li> <li>Use code blocks with language identifiers for syntax highlighting</li> <li>Use blockquotes (<code>&gt;</code>) for important notes or warnings</li> <li>Use consistent formatting for code snippets, file paths, and commands</li> </ul>"},{"location":"admin-guides/editing-digital-archive-handbook.html#content-organization","title":"Content Organization","text":"<ul> <li>Keep pages focused on a single topic</li> <li>Use consistent terminology throughout</li> <li>Cross-reference related pages using relative links</li> <li>Update the navigation in <code>mkdocs.yml</code> when adding new sections</li> </ul>"},{"location":"admin-guides/editing-digital-archive-handbook.html#writing-style","title":"Writing Style","text":"<ul> <li>Write in clear, concise language</li> <li>Use active voice when possible</li> <li>Include step-by-step instructions for procedures</li> <li>Add context and purpose for technical procedures</li> </ul>"},{"location":"admin-guides/editing-digital-archive-handbook.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guides/editing-digital-archive-handbook.html#common-issues","title":"Common Issues","text":"<p>MkDocs serve not working: - Ensure the virtual environment is activated - Check that MkDocs is installed: <code>pip list | grep mkdocs</code> - Try rebuilding: <code>mkdocs build</code> then <code>mkdocs serve</code></p> <p>Changes not appearing: - Hard refresh your browser (Ctrl+F5 or Cmd+Shift+R) - Check that you're editing files in <code>/docs/</code>, not <code>/site/</code> - Verify the file is included in <code>mkdocs.yml</code> navigation</p> <p>Build errors: - Check Markdown syntax for errors - Verify all linked files exist - Check <code>mkdocs.yml</code> for syntax errors (YAML is sensitive to indentation)</p> <p>Images not displaying: - Verify the image path is correct relative to the Markdown file - Check that the image file exists in <code>/docs/assets/images/</code> - Ensure the image file is committed to Git</p>"},{"location":"admin-guides/editing-digital-archive-handbook.html#additional-resources","title":"Additional Resources","text":"<ul> <li>MkDocs Documentation</li> <li>Material for MkDocs Documentation</li> <li>Markdown Guide</li> <li>Git Documentation</li> </ul>"},{"location":"admin-guides/reporting.html","title":"Reporting / Statistics","text":"<p>Purpose: Methods used to generate end of year reports and statistics.</p> <p>Note: This page is under development. Future additions will include: - Report Types section - Schedule section - Access section - Format section - Distribution section - Archival section</p>"},{"location":"admin-guides/reporting.html#preservica","title":"Preservica","text":"<p>Purpose: A count of assets needs to be calculated at the end of the year. For example, at the end of 2021, Preservica contained 12,600 assets - by the end of 2022, Preservica contained 18,428 assets. This implies that the digital archive has grown by 5,768 assets.</p> <p>Process: To get a number of assets, a custom script needs to be run which exports to CSV a list of all assets and folders. The CSV file contains a column which designates whether an item is of <code>EntityType.ASSET</code> or <code>EntityType.FOLDER</code>. <code>EntityType.FOLDER</code> needs to be filtered out to get an accurate asset count.</p>"},{"location":"admin-guides/reporting.html#web-archives-archive-it","title":"Web archives / Archive-It","text":"<p>Note: Under construction \ud83d\udc77 Still constructing</p>"},{"location":"admin-guides/preservica/appraisal.html","title":"Appraisal","text":"<p>Purpose: Appraisal is the decision to ingest into Preservica. During this decision we want to check if: - The resource is of value to the Digital Repository - The resource will not impact the health of the Digital Repository - We have permission to ingest the resource into the Digital Repository</p> <p>Note: Would be handy to have a collections guide here.</p>"},{"location":"admin-guides/preservica/appraisal.html#virus-check","title":"Virus check","text":"<p>Purpose: It is important that we ensure the ongoing health of the Digital Repository. During the appraisal workflow we achieve this by running Brunnhilde. Brunnhilde also provides an overview of the resources we are planning to ingest, helping to spot any problem files before ingest.</p>"},{"location":"admin-guides/preservica/appraisal.html#brunnhilde","title":"Brunnhilde","text":"<p>Brunnhilde provides the following features:</p> <ul> <li>Virus check</li> <li>File analysis</li> <li>Checksums</li> </ul> <p>Command: <pre><code>brunnhilde.py source destination\n</code></pre></p> <p>Parameters: - <code>source</code> - Path to source directory or disk image - <code>destination</code> - Path to destination for reports</p> <p>Tip: Check for any complications such as 0-sized files and duplicates.</p>"},{"location":"admin-guides/preservica/appraisal.html#check-the-gdrive","title":"Check the G:Drive","text":"<p>Purpose: When appraising a collection of documents, it is important to check for similar resources in the G:Drive from the VDI.</p> <p>Location: <pre><code>G:\\MD\\LIS\\LIBRARY AND INFORMATION SERVICE\\ICAEW digital repository\n</code></pre></p> <p>Note: The G:Drive is a place where documents have already been sorted into hierarchies. They may not always have a full collection but can be useful for laying the groundwork to create a series in Preservica.</p> <p>When searching the G:Drive: - Look for duplicates and additional resources for the collections you are appraising - If duplicates are found, remove them from the G:Drive after ingest - If additional resources are found, transfer them from the VDI onto your local machine using the Digital Archive Sharepoint</p>"},{"location":"admin-guides/preservica/appraisal.html#creating-a-series","title":"Creating a series","text":"<p>Note: Unfortunately there is no one-size-fits-all approach for creating an archival series. It is important to keep in mind good archival practice.</p>"},{"location":"admin-guides/preservica/appraisal.html#guidelines","title":"Guidelines","text":"<ol> <li>Respect Provenance</li> <li>Maintain original structure where possible, unless it compromises usability or access</li> <li> <p>Group resources that have the same point of origin</p> </li> <li> <p>Define the Purpose of the Series</p> </li> <li> <p>Why is this series being collated? Some examples:</p> <ul> <li>Faculty Publications</li> <li>Annual Reports</li> <li>Press Releases</li> </ul> </li> <li> <p>Avoid Overly Deep Hierarchies</p> </li> <li>The deeper a hierarchy, the harder it is to manage and navigate</li> <li> <p>Try to keep the hierarchy manageable. Some examples:</p> <ul> <li><code>Root/Faculty Serials/Individual Serial/Year</code></li> <li><code>Root/Press Releases/Year</code></li> <li><code>Root/Business Confidence Monitor/BCM Sectors/Individual Sector</code></li> </ul> </li> <li> <p>Keep Series Intellectually Cohesive</p> </li> <li>Avoid mixing formats or topics. Only consider doing this if they were originally grouped that way</li> <li> <p>If digital assets span many types, consider separating them into sub-series</p> </li> <li> <p>Work Collaboratively</p> </li> <li>Often the Digital Repository won't have the best understanding of the resources we appraise</li> <li>Work collaboratively with information stakeholders to best understand the resources and build the structure with their help</li> </ol> <p>Next Step: After appraisal and creating a series structure for the resources, it is time to begin ingesting the resources.</p>"},{"location":"admin-guides/preservica/archived-pages.html","title":"Archived Pages","text":"<p>Purpose: This document contains reference information for various Preservica processes and workflows. For current workflows, see the dedicated pages in this section.</p> <p>Note: This page contains legacy and reference information. Some sections may be duplicated or outdated. Please refer to the main workflow pages for current processes.</p>"},{"location":"admin-guides/preservica/archived-pages.html#appraisal-tools","title":"Appraisal Tools","text":"<p>[NOTE: Add a \"Prerequisites\" section] [NOTE: Add a \"Assessment Criteria\" section] [NOTE: Add a \"Tools and Methods\" section] [NOTE: Add a \"Documentation\" section] [NOTE: Add a \"Decision Making\" section] [NOTE: Add a \"Best Practices\" section]</p>"},{"location":"admin-guides/preservica/archived-pages.html#brunnhilde","title":"Brunnhilde","text":"<p>Brunnhilde provides the three following features:</p> <ul> <li>virus check</li> <li>file analysis</li> <li> <p>checksums</p> </li> <li> <p>Run Brunnhilde using the following in cmd:</p> <pre><code>    brunnhilde.py source destination\n</code></pre> </li> <li> <p><code>source</code> being the path to source directory or disk image <code>destination</code> being the path to destination for reports   Check for any complications such as 0 sized files and duplicates.</p> </li> </ul>"},{"location":"admin-guides/preservica/archived-pages.html#exiftool","title":"ExifTool","text":"<ul> <li>Exiftool lists the properties of a document in a csv to be human readable. The following properties will be included: title, author, subject, keywords, producer, CreateDate, ModifyDate, FileName.</li> <li> <p>Check the title properties, if the titles are misleading or completely incorrect. Strip the metadata from the file if any are found to be misleading or completely incorrect.</p> </li> <li> <p>Run Exiftool using the following in cmd:</p> <pre><code>exiftool -csv -r -T -Title -Author -Subject -Keywords -Producer -CreateDate -ModifyDate -FileName \"input file/folder\" &gt; \"output.csv\"\n</code></pre> </li> </ul> <p>TODO: Cover how to strip metadata from PDFs, doc, docx etc. here.</p>"},{"location":"admin-guides/preservica/archived-pages.html#archived-pages_1","title":"Archived Pages","text":"<p>Purpose: This document contains reference information for various Preservica processes and workflows. For current workflows, see the dedicated pages in this section.</p> <p>Note: This page contains legacy and reference information. Some sections may be duplicated or outdated. Please refer to the main workflow pages for current processes.</p>"},{"location":"admin-guides/preservica/archived-pages.html#appraisal-tools_1","title":"Appraisal Tools","text":"<p>[NOTE: Add a \"Prerequisites\" section] [NOTE: Add a \"Assessment Criteria\" section] [NOTE: Add a \"Tools and Methods\" section] [NOTE: Add a \"Documentation\" section] [NOTE: Add a \"Decision Making\" section] [NOTE: Add a \"Best Practices\" section]</p>"},{"location":"admin-guides/preservica/archived-pages.html#brunnhilde_1","title":"Brunnhilde","text":"<p>Brunnhilde provides the three following features:</p> <ul> <li>virus check</li> <li>file analysis</li> <li> <p>checksums</p> </li> <li> <p>Run Brunnhilde using the following in cmd:</p> <pre><code>    brunnhilde.py source destination\n</code></pre> </li> <li> <p><code>source</code> being the path to source directory or disk image <code>destination</code> being the path to destination for reports   Check for any complications such as 0 sized files and duplicates.</p> </li> </ul>"},{"location":"admin-guides/preservica/archived-pages.html#exiftool_1","title":"ExifTool","text":"<ul> <li>Exiftool lists the properties of a document in a csv to be human readable. The following properties will be included: title, author, subject, keywords, producer, CreateDate, ModifyDate, FileName.</li> <li> <p>Check the title properties, if the titles are misleading or completely incorrect. Strip the metadata from the file if any are found to be misleading or completely incorrect.</p> </li> <li> <p>Run Exiftool using the following in cmd:</p> <pre><code>exiftool -csv -r -T -Title -Author -Subject -Keywords -Producer -CreateDate -ModifyDate -FileName \"input file/folder\" &gt; \"output.csv\"\n</code></pre> </li> </ul> <p>TODO: Cover how to strip metadata from PDFs, doc, docx etc. here.</p>"},{"location":"admin-guides/preservica/archived-pages.html#auto-classification-for-dc-metadata","title":"Auto Classification for DC Metadata","text":"<p>Purpose: ICAEW uses Smartlogic Semaphore to auto-classify material using a custom made taxonomy. The Classification &amp; Language Service Client tool provides subject classification for documents stored locally.</p> <p>Note: This section is under development. Future additions will include: - Overview section - Configuration section - Rules Setup section - Process Flow section - Validation section - Best Practices section</p> <p>Configuration: We have decided to include only subjects with a classification threshold of 48% and above - this matches what is happening on ICAEW.com. Topics can be removed at the discretion of the digital archivist if they feel the document has been assigned too many subjects.</p> <p>Usage: This tool will mostly be used in conjunction with the semaphore-helper.py script.</p> <p>Credentials: <code>--cloud-api-key</code> can be found on the Logins page.</p> <p>CLI example: <pre><code>java -jar Semaphore-CLSClient-5.6.3.jar --cloud-api-key= --url=https://cloud.smartlogic.com/svc/138b5bab-8ac4-45e0-b36f-815008f9921d/ --threshold=48 --csv-output-file subject-terms-output.csv input\n</code></pre></p> <p>Things to note: - <code>threshold=48</code> is the 48% classification threshold - The output must be a CSV - We use the terms from the Generic_UPWARD column</p>"},{"location":"admin-guides/preservica/archived-pages.html#the-classification-language-service-client","title":"The Classification &amp; Language Service Client","text":"<p>Reference: The Classification &amp; Language Service Client tool can be downloaded from the Smartlogic portal, along with the documentation: CLS-Client</p>"},{"location":"admin-guides/preservica/archived-pages.html#post-ingest","title":"Post-Ingest","text":"<p>Reference: For current post-ingest processes, see Post Ingest.</p> <p>Note: This section is under development. Future additions will include: - Verification Steps section - Quality Checks section - Metadata Review section - Access Testing section - Documentation section - Troubleshooting section</p>"},{"location":"admin-guides/preservica/archived-pages.html#update-catalogue-entries","title":"Update Catalogue entries","text":"<p>Purpose: Post-ingest, the ICAEW catalogue needs updating to include the new access point in Preservica. This is achieved by adding a new 856 field in Symphony.</p> <p>Formatting for adding an 856 field in Symphony: <pre><code>|aAvailable at the ICAEW Digital Repository: |u*hyperlink*\n</code></pre></p> <p>Note: (To be discussed) If we add unique identifiers alongside Dublin Core metadata, these should be added into the catalogue as well.</p> <p>TODO: Write post-ingest processes</p>"},{"location":"admin-guides/preservica/archived-pages.html#appendix-legacy-processes","title":"Appendix: Legacy Processes","text":"<p>Note: Processes that are no longer used.</p> <ul> <li>Semaphore CLS Client and semaphore-subject-import.py</li> </ul> <p>Purpose: The semaphore-subject-import.py script combines the CSV output from the Semaphore CLS Client, negating the need for copy and pasting from one CSV to the other.</p> <p>Usage: <pre><code>semaphore-subject-import.py semaphore_csv dublin_core_csv csv_output\n</code></pre></p>"},{"location":"admin-guides/preservica/archived-pages.html#pre-ingest","title":"Pre-Ingest","text":"<p>Reference: For current pre-ingest processes, see Ingest Workflow and Standard Ingest Workflow.</p>"},{"location":"admin-guides/preservica/archived-pages.html#prerequisites","title":"Prerequisites","text":"<p>Required: - Python 3.x environment with required packages - Access to Preservica API credentials - Understanding of Dublin Core metadata standards - Familiarity with OPEX package structure</p>"},{"location":"admin-guides/preservica/archived-pages.html#file-preparation","title":"File Preparation","text":"<ol> <li>Organize files in a clean directory structure</li> <li>Ensure files follow naming conventions</li> <li>Remove any temporary or system files</li> <li>Verify file integrity and readability</li> </ol>"},{"location":"admin-guides/preservica/archived-pages.html#metadata-creation","title":"Metadata Creation","text":"<p>Purpose: The pre-ingest process uses a series of Python scripts to create Submission Information Packages (SIPs):</p>"},{"location":"admin-guides/preservica/archived-pages.html#1-generate-initial-csv-a_files_to_csvpy","title":"1. Generate Initial CSV (<code>a_files_to_csv.py</code>)","text":"<p><pre><code>python3 a_files_to_csv.py [-h] [--hash_type {sha1,md5}] directory output\n</code></pre> This script: - Creates a CSV file with SHA-1/MD5 hashes for each artifact - Includes Dublin Core fields for .opex metadata - Preserves original file information</p>"},{"location":"admin-guides/preservica/archived-pages.html#2-auto-classification-semaphore-helperpy","title":"2. Auto-classification (<code>semaphore-helper.py</code>)","text":"<p><pre><code>python3 semaphore-helper.py [-h] [--preservica_ref PRESERVICA_REF] directory\n</code></pre> This script: - Uses Semaphore's Classification Server - Outputs up to 10 subject topics - Sorts by topic score - Accepts local directory or Preservica folder reference</p>"},{"location":"admin-guides/preservica/archived-pages.html#3-file-renaming-csv_file_renamepy","title":"3. File Renaming (<code>csv_file_rename.py</code>)","text":"<p><pre><code>python3 csv_file_rename.py file_directory csv_file_path\n</code></pre> This script: - Renames files based on Title field from CSV - Preserves file extensions - Updates CSV with new filenames</p>"},{"location":"admin-guides/preservica/archived-pages.html#4-generate-opex-files-b_csv_to_opex_xmlpy","title":"4. Generate OPEX Files (<code>b_csv_to_opex_xml.py</code>)","text":"<p><pre><code>python3 b_csv_to_opex_xml.py csv_file output_dir\n</code></pre> This script: - Creates unique .opex metadata files - Includes fixity and Dublin Core metadata - Validates XML structure</p>"},{"location":"admin-guides/preservica/archived-pages.html#5-create-folder-opex-c_folders_of_files_to_folder_opex_xmlpy","title":"5. Create Folder OPEX (<code>c_folders_of_files_to_folder_opex_xml.py</code>)","text":"<p><pre><code>python3 c_folders_of_files_to_folder_opex_xml.py folder_path\n</code></pre> This script: - Creates folder-level .opex metadata - Generates manifest for ingest - Allows manual editing of metadata</p>"},{"location":"admin-guides/preservica/archived-pages.html#quality-checks","title":"Quality Checks","text":"<ol> <li>Verify all files have corresponding .opex files</li> <li>Check metadata completeness</li> <li>Validate XML structure</li> <li>Confirm file integrity</li> <li>Review folder structure</li> </ol>"},{"location":"admin-guides/preservica/archived-pages.html#validation","title":"Validation","text":"<ul> <li>Run XML validation on all .opex files</li> <li>Verify checksums match</li> <li>Check Dublin Core compliance</li> <li>Test folder structure integrity</li> </ul>"},{"location":"admin-guides/preservica/archived-pages.html#best-practices","title":"Best Practices","text":"<ol> <li>Always work in a clean directory</li> <li>Keep original files backed up</li> <li>Document any manual changes</li> <li>Use consistent naming conventions</li> <li>Validate at each step</li> <li>Test with a small batch first</li> </ol>"},{"location":"admin-guides/preservica/archived-pages.html#troubleshooting","title":"Troubleshooting","text":"<p>Common issues and solutions: 1. Missing metadata fields 2. Invalid XML structure 3. Checksum mismatches 4. File permission issues 5. API connection problems</p>"},{"location":"admin-guides/preservica/archived-pages.html#support","title":"Support","text":"<p>For issues or questions, contact the Digital Archive team.</p>"},{"location":"admin-guides/preservica/archived-pages.html#prepare-a-submission-information-package-sip","title":"Prepare a Submission Information Package (SIP)","text":"<p>Purpose: Creating a SIP requires: Fixity, Dublin Core descriptive metadata and the digital artefacts.</p> <p>Note:  - For Fixity, ICAEW uses the SHA-1 algorithm - Refer to the Dublin Core page to understand how ICAEW uses it</p> <p>SIP Structure: - Each of these elements are placed in a folder, the fixity and descriptive metadata are nested in an .opex metadata template beside the digital artefact they represent - Each artefact including the folder must have a separate .opex metadata document - The folder level .opex metadata document is unique to the other .opex files, its primary role is to include a manifest of the folder: both content and metadata</p>"},{"location":"admin-guides/preservica/archived-pages.html#python-sip-creation-tools","title":"Python SIP creation tools","text":"<p>Purpose: We have multiple tools that achieve the objective of creating a Preservica SIP. The tools can be found on the icaew-digital-archive GitHub page.</p> <p>Tool A: Creates a .csv file with separate SHA-1 hashes for each artefact and includes relevant Dublin Core fields to populate for the .opex metadata file.</p> <p>Command: <pre><code>a_files_to_csv.py [-h] [--hash_type {sha1,md5}] directory output\n</code></pre></p> <p>Note: Populate the .csv with the relevant Dublin Core metadata - leaving the Subject fields for the next stage of the process.</p> <p>Semaphore auto-classification (via semaphore-helper.py):</p> <p>Command: <pre><code>semaphore-helper.py [-h] [--preservica_ref PRESERVICA_REF] directory\n</code></pre></p> <p>Purpose: Make use of Semaphore's Classification Server via semaphore-helper.py. This script acts as a wrapper around the Semaphore CLS client and outputs a maximum of 10 subject topics, sorted by scoring. The script accepts a local directory or a Preservica folder reference can be supplied directly to the script. Further information regarding the Semaphore CLS client can be found here. After populating the .csv with the relevant Dublin Core metadata, move on to the rename tool.</p> <p>Rename tool: Renames the original filenames (found in the 'filename' column) to filenames given in the 'Title' column from the csv produced by a_files_to_csv.py. Use the output CSV file from semaphore-subject-import.py (unless the step was skipped for whatever reason).</p> <p>Command: <pre><code>csv_file_rename.py file_directory csv_file_path\n</code></pre></p> <p>Important: After running the script, you should also copy the contents of the 'Title' field over to 'filename' in the csv file as Tool B uses this column to name the .opex files. You will also need to re-add the file extensions if following this method.</p> <p>Tool B: Uses the completed .csv to create unique .opex metadata files for each digital artefact in the folder. Including fixity and Dublin Core metadata.</p> <p>Command: <pre><code>b_csv_to_opex_xml.py csv_file output_dir\n</code></pre></p> <p>Next Step: After this tool has successfully created a .opex metadata file for each digital artefact, move on to Tool C.</p> <p>Tool C: Creates a folder level .opex metadata document. It scans the parent folder and creates an authenticated folder level .opex metadata document that serves as a manifest for the ingest. The folder level opex file can then be edited before ingest if needed, i.e. adding/changing the Title, Description, SecurityDescriptor and other Dublin Core metadata.</p> <p>Command: <pre><code>c_folders_of_files_to_folder_opex_xml.py folder_path\n</code></pre></p>"},{"location":"admin-guides/preservica/archived-pages.html#reclassification","title":"Reclassification","text":"<p>Purpose: The following document outlines the reclassification workflow.</p> <p>Note: This section is under development. Future additions will include: - Prerequisites section - Classification Rules section - Process Steps section - Validation section - Error Handling section - Best Practices section</p>"},{"location":"admin-guides/preservica/archived-pages.html#overview","title":"Overview","text":"<p>Purpose: The reclassification workflow downloads assets from the ICAEW Preservica digital archive. Then using Python tools and the Semaphore taxonomy, each asset is re-classified with Dublin Core metadata. Upon completing the Dublin Core metadata CSV, the existing metadata connected to the asset will be deleted. The new metadata will then be connected to the asset. Four tools are used in the workflow, the workflow has two stages.</p> <p>Prerequisites: Each of these tools require that the operator is within a virtual environment in the relevant directory.</p> <p>Activate virtual environment: <pre><code>. ./venv/bin/activate\n</code></pre></p>"},{"location":"admin-guides/preservica/archived-pages.html#download-and-classify","title":"Download and Classify","text":"<p>Tools used: - <code>a_get_metadata.py</code> - <code>semaphore-helper.py</code></p> <p>Purpose: During this stage, assets and existing metadata will be downloaded from Preservica. Using semaphore, each of these assets will be re-classified and any additional metadata will be added/updated.</p>"},{"location":"admin-guides/preservica/archived-pages.html#a_get_metadatapy","title":"a_get_metadata.py","text":"<p>Purpose: The first step is to download the existing metadata for the assets that are to be reclassified. First step is to grab the asset or folder ID from Preservica. Secondly, create a CSV. This will be where the relevant Dublin Core metadata is added/updated.</p> <p>Command: <pre><code>python3 a_get_metadata.py [preservica reference number] [output.csv]\n</code></pre></p> <p>Warning: The CSV output will require manual column realignment if there are repeating Dublin Core elements.</p>"},{"location":"admin-guides/preservica/archived-pages.html#semaphore-helperpy","title":"semaphore-helper.py","text":"<p>Purpose: The semaphore-helper script is a multi-part tool that firstly downloads a copy of each required asset from Preservica and then using the ICAEW taxonomy classifies up to ten subject terms for the Dublin Core subject fields.</p> <p>Requirements: - Preservica reference number - Area for each asset to be downloaded to - Output preferably CSV</p> <p>Command: <pre><code>python3 semaphore-helper.py [preservica ref] [asset download location] [output location &gt; .csv]\n</code></pre></p>"},{"location":"admin-guides/preservica/archived-pages.html#delete-and-upload","title":"Delete and Upload","text":"<p>Tools used: - <code>b_delete_metadata.py</code> - <code>c_add_metadata_from_csv.py</code></p> <p>Purpose: During this stage, existing metadata will be deleted from the enclosed Preservica environment. Afterwards the new completed Dublin Core metadata will be added to the assets.</p>"},{"location":"admin-guides/preservica/archived-pages.html#b_delete_metadatapy","title":"b_delete_metadata.py","text":"<p>Purpose: The <code>b_delete_metadata.py</code> permanently deletes metadata templates connected to the asset. It uses a Preservica reference number to locate which assets' metadata it will delete. Using a folder's Preservica reference number will delete each of its child files' metadata templates.</p> <p>Command: <pre><code>python3 b_delete_metadata.py [preservica_ref]\n</code></pre></p> <p>Warning: Each time this script is used, a Y/N prompt occurs. After Y, the metadata is permanently deleted.</p>"},{"location":"admin-guides/preservica/archived-pages.html#c_add_metadata_from_csvpy","title":"c_add_metadata_from_csv.py","text":"<p>Purpose: Lastly, the <code>c_add_metadata_from_csv.py</code> tool adds the newly complete Dublin Core metadata CSV to the original assets within Preservica.</p> <p>Command: <pre><code>python3 c_add_metadata_from_csv.py [csv_location]\n</code></pre></p> <p>Note: This tool uses the Preservica reference numbers in the CSV to locate where to add the newly completed Dublin Core metadata template.</p>"},{"location":"admin-guides/preservica/aws-cli.html","title":"AWS CLI","text":""},{"location":"admin-guides/preservica/aws-cli.html#overview","title":"Overview","text":"<p>Purpose: The AWS Command Line Interface (CLI) is a unified tool to manage AWS services. In the context of digital archiving, it's primarily used for uploading large files to Preservica's PUT area. The CLI provides a more reliable and efficient way to handle large file transfers compared to web interfaces.</p>"},{"location":"admin-guides/preservica/aws-cli.html#installation","title":"Installation","text":"<ol> <li>Download the AWS CLI from the official website</li> <li> <p>Follow the installation instructions for your operating system:</p> <ul> <li>Linux: Use package manager or install from source</li> </ul> </li> </ol>"},{"location":"admin-guides/preservica/aws-cli.html#configuration","title":"Configuration","text":"<p>After installation, configure the AWS CLI with your Preservica credentials:</p> <p>Command: <pre><code>aws configure\n</code></pre></p> <p>You'll need to enter: - AWS Access Key ID (from Preservica Logins page) - AWS Secret Access Key (from Preservica Logins page) - Default region name (use the one specified in Preservica) - Default output format (json or text)</p>"},{"location":"admin-guides/preservica/aws-cli.html#usage-examples","title":"Usage Examples","text":""},{"location":"admin-guides/preservica/aws-cli.html#list-directory-contents","title":"List Directory Contents","text":"<pre><code>aws s3 ls s3://com.preservica.icaew.put.holding/    </code></pre>"},{"location":"admin-guides/preservica/aws-cli.html#upload-a-single-file","title":"Upload a Single File","text":"<pre><code>aws s3 cp '/home/digital-archivist/Downloads/20220719-ICAEW-com-media-library.zip' s3://com.preservica.icaew.put.holding/20220719-ICAEW-com-media-library.zip\n</code></pre>"},{"location":"admin-guides/preservica/aws-cli.html#upload-a-directory","title":"Upload a Directory","text":"<pre><code>aws s3 cp --recursive local_directory s3://com.preservica.icaew.put.holding/NewFolder/\n</code></pre>"},{"location":"admin-guides/preservica/aws-cli.html#list-objects-with-metadata","title":"List Objects with Metadata","text":"<pre><code>aws s3api list-objects --bucket com.preservica.icaew.export\n</code></pre>"},{"location":"admin-guides/preservica/backing-up.html","title":"Backing Up","text":"<p>Purpose: This document outlines the process for creating local backups of Preservica content.</p> <p>Note: This page is under development. Future additions will include: - Overview section - Backup Types section - Schedule section - Storage section - Verification section - Best Practices section</p>"},{"location":"admin-guides/preservica/backing-up.html#running-the-report-and-downloading-the-files","title":"Running the report and downloading the files","text":"<p>Purpose: Local backups of the entire contents of Preservica should be made at least on a yearly basis.</p> <p>Process: - There is already a workflow setup for this purpose called Full Export (v6) which is accessible from the Access and then Manage tab - To run the full export, the workflow needs to be made active by clicking the tickbox - The download will be made available at: <code>com.preservica.icaew.export</code> - Access and downloading will be via aws-cli as usual</p> <p>Important: Preservica should be contacted before running this report as it is apparently process heavy. It should be run over the weekend to minimise any potential disruption to their servers.</p>"},{"location":"admin-guides/preservica/backing-up.html#unzipping-the-preservica-download","title":"Unzipping the Preservica download","text":"<p>Purpose: The Preservica backup download will be contained within .zip files, which in turn contain further .zip files (i.e., nested .zip files). In order to make the files more accessible, to generate local hashes, and for tools such as Brunnhilde/ssdeep to work as intended, the contents must be unzipped.</p> <p>Warning: The following script deletes the .zip source files while recursively unzipping them. This script should only be run on a copy of the Preservica download.</p> <p>Bash script to recursively unzip nested .zip files (found here). The script should be run at the location of the .zip files:</p> <pre><code>while [[ -n $(find . -name '*.zip') ]]\ndo\nfind . -name '*.zip'|awk -F'.zip' '{print \"unzip -d \"$1\" \"$0\" &amp;&amp; rm \"$0\"\"}'|sh\ndone\n</code></pre>"},{"location":"admin-guides/preservica/downloading-streamamg-videos.html","title":"Downloading StreamAMG Videos","text":"<p>Purpose: This guide provides instructions for downloading videos from StreamAMG and exporting metadata in bulk.</p>"},{"location":"admin-guides/preservica/downloading-streamamg-videos.html#prerequisites","title":"Prerequisites","text":"<p>Required: - Access to KMC Management Console - StreamAMG API credentials - Python 3.x environment - Required Python packages installed</p>"},{"location":"admin-guides/preservica/downloading-streamamg-videos.html#access-requirements","title":"Access Requirements","text":"<p>Required: - Valid StreamAMG account with appropriate permissions - API access credentials (secret and partnerId) - Sufficient storage space for video downloads</p>"},{"location":"admin-guides/preservica/downloading-streamamg-videos.html#download-process","title":"Download Process","text":""},{"location":"admin-guides/preservica/downloading-streamamg-videos.html#1-kmc-management-console-setup","title":"1. KMC Management Console Setup","text":"<p>Steps: 1. Log in to KMC Management Console 2. Navigate to Integration Settings 3. Note down your secret and partnerId</p>"},{"location":"admin-guides/preservica/downloading-streamamg-videos.html#2-video-download","title":"2. Video Download","text":"<p>Steps: 1. Access video library in KMC 2. Select videos for download 3. Use \"MORE ACTIONS\" \u2192 \"Download\" 4. Choose \"Source\" format 5. Save download link when received</p>"},{"location":"admin-guides/preservica/downloading-streamamg-videos.html#3-metadata-export","title":"3. Metadata Export","text":"<p>Purpose: The process uses the xml_to_csv.py script to process metadata.</p> <p>Steps:</p> <ol> <li>Obtain API Session:</li> <li>Use API Test Console</li> <li>Select \"session\" service</li> <li>Choose \"start\" action</li> <li>Enter credentials</li> <li>Set type to \"ADMIN\"</li> <li> <p>Generate KS string</p> </li> <li> <p>Retrieve Metadata:</p> </li> <li>Select \"media\" service</li> <li>Choose \"list\" action</li> <li>Set pageSize to 500 (maximum)</li> <li>Increment pageIndex for multiple pages</li> <li> <p>Save XML output</p> </li> <li> <p>Process Metadata: <pre><code>python3 xml_to_csv.py --xml_file input.xml --csv_file output.csv\n</code></pre></p> </li> </ol>"},{"location":"admin-guides/preservica/downloading-streamamg-videos.html#quality-settings","title":"Quality Settings","text":"<p>Best Practices: - Download original source files - Preserve original metadata - Maintain video quality - Include all associated files</p>"},{"location":"admin-guides/preservica/downloading-streamamg-videos.html#error-handling","title":"Error Handling","text":"<p>Common issues and solutions: 1. API Connection:    - Verify credentials    - Check network connection    - Validate KS string</p> <ol> <li>Download Issues:</li> <li>Check storage space</li> <li>Verify file permissions</li> <li> <p>Monitor download progress</p> </li> <li> <p>Metadata Processing:</p> </li> <li>Validate XML structure</li> <li>Check CSV output</li> <li>Verify field mapping</li> </ol>"},{"location":"admin-guides/preservica/downloading-streamamg-videos.html#best-practices","title":"Best Practices","text":"<ol> <li>Always download source quality</li> <li>Keep original filenames</li> <li>Maintain metadata integrity</li> <li>Document any issues</li> <li>Test with single video first</li> <li>Verify downloads before processing</li> </ol>"},{"location":"admin-guides/preservica/downloading-streamamg-videos.html#support","title":"Support","text":"<p>For issues or questions, contact the Digital Archive team.</p>"},{"location":"admin-guides/preservica/downloading-streamamg-videos.html#streamamg-video-downloads-and-metadata-export-guide","title":"StreamAMG Video Downloads and Metadata Export Guide","text":"<p>Purpose: This guide provides clear, step-by-step instructions on how to download videos from StreamAMG and efficiently export metadata in bulk using StreamAMG's API and the xml-to-csv.py script.</p>"},{"location":"admin-guides/preservica/downloading-streamamg-videos.html#key-resources","title":"Key Resources","text":"<p>Below are key links referenced throughout this guide:</p> <ul> <li>KMC Management Console</li> <li>API Test Console</li> <li>API Documentation</li> </ul>"},{"location":"admin-guides/preservica/downloading-streamamg-videos.html#part-1-downloading-videos-from-streamamg","title":"Part 1: Downloading Videos from StreamAMG","text":"<p>Follow the steps below to download videos from StreamAMG:</p>"},{"location":"admin-guides/preservica/downloading-streamamg-videos.html#1-log-in-to-the-kmc-management-console","title":"1. Log in to the KMC Management Console","text":"<ul> <li>Access the KMC Management Console and log in with your credentials.</li> </ul>"},{"location":"admin-guides/preservica/downloading-streamamg-videos.html#2-navigate-to-the-video-library","title":"2. Navigate to the Video Library","text":"<ul> <li>Once logged in, go to the video library and select the videos you wish to download.</li> </ul>"},{"location":"admin-guides/preservica/downloading-streamamg-videos.html#3-initiate-the-download","title":"3. Initiate the Download","text":"<ul> <li>With your videos selected, click on the \"MORE ACTIONS\" dropdown menu and choose the \"Download\" option.</li> </ul>"},{"location":"admin-guides/preservica/downloading-streamamg-videos.html#4-choose-the-download-format","title":"4. Choose the Download Format","text":"<ul> <li>In the download options, select \"Source\" to download the original video file.</li> </ul>"},{"location":"admin-guides/preservica/downloading-streamamg-videos.html#5-complete-the-download","title":"5. Complete the Download","text":"<ul> <li>You will receive an email with a download link. Right-click on the link and select \"Save link as...\" to save the file to your local device.</li> </ul>"},{"location":"admin-guides/preservica/downloading-streamamg-videos.html#part-2-exporting-streamamg-metadata","title":"Part 2: Exporting StreamAMG Metadata","text":"<p>Tip: For best results, use Firefox for this process. The XML output might not render correctly in Chrome.</p> <p>This process involves using the API Test Console and the xml-to-csv.py script.</p>"},{"location":"admin-guides/preservica/downloading-streamamg-videos.html#1-obtain-a-ks-string","title":"1. Obtain a \"KS\" String","text":"<ul> <li>Open the API Test Console.</li> <li>From the \"Select service\" dropdown, choose \"session\".</li> <li>From the \"Select action\" dropdown, choose \"start\".</li> <li>Enter your \"secret\" and \"partnerId\" (these can be found in the Integration Settings in the KMC Management Console).</li> <li>Set the \"type (KalturaSessionType)\" to \"ADMIN\".</li> <li>Click \"Submit\" to generate a \"KS\" string, which will be used in the following steps.</li> </ul>"},{"location":"admin-guides/preservica/downloading-streamamg-videos.html#2-retrieve-metadata","title":"2. Retrieve Metadata","text":"<ul> <li>In the same API Test Console session, select \"media\" from the \"Select service\" dropdown and \"list\" from the \"Select action\" dropdown.</li> <li>Tick and edit the \"pager (KalturaFilterPager)\".</li> <li>Enter \"500\" in the \"pageSize\" (this is the maximum allowed).</li> <li>Enter \"1\" in the \"pageIndex\" for the first page of results. For subsequent pages, increment this number (e.g., 2, 3, 4).</li> </ul>"},{"location":"admin-guides/preservica/downloading-streamamg-videos.html#3-save-the-metadata-xml","title":"3. Save the Metadata XML","text":"<ul> <li>The results will appear as an XML file. Right-click within the frame containing the XML, select \"This Frame,\" and then choose \"Save Frame As...\" to save the XML file.</li> <li>Repeat this process for each subsequent page of metadata.</li> </ul>"},{"location":"admin-guides/preservica/downloading-streamamg-videos.html#4-convert-xml-to-csv","title":"4. Convert XML to CSV","text":"<ul> <li>Once you have saved the XML files, use the xml-to-csv.py script to convert them to CSV format.</li> <li>The script requires specifying an <code>xml_file</code> as input and a <code>csv_file</code> as output.</li> </ul>"},{"location":"admin-guides/preservica/downloading-youtube-videos-api.html","title":"YouTube Video Downloads and Metadata Export Guide","text":"<p>Purpose: This guide outlines the process for downloading videos and exporting metadata from a YouTube Brand Account using the YouTube Data API.</p> <p>Note: This page is under development. Future additions will include: - Prerequisites section - API Setup section - Download Process section - Quality Settings section - Error Handling section - Best Practices section</p>"},{"location":"admin-guides/preservica/downloading-youtube-videos-api.html#overview","title":"Overview","text":"<p>Summary: Here's a summary of the entire process, from creating the app in Google Cloud to running the script to get a list of videos from a YouTube Brand Account you manage:</p>"},{"location":"admin-guides/preservica/downloading-youtube-videos-api.html#1-set-up-the-google-cloud-project","title":"1. Set Up the Google Cloud Project","text":"<ol> <li> <p>Go to Google Developers Console:</p> <ul> <li>Visit the Google Developers Console.</li> <li>Sign in with your Google account.</li> </ul> </li> <li> <p>Create a New Project:</p> <ul> <li>Click Select a project in the top-right corner.</li> <li>Click New Project, give it a name (e.g., \"YouTube Data API Project\"), and click Create.</li> </ul> </li> <li> <p>Enable YouTube Data API v3:</p> <ul> <li>In your project, click on Enable APIs and Services.</li> <li>Search for YouTube Data API v3, select it, and click Enable.</li> </ul> </li> </ol>"},{"location":"admin-guides/preservica/downloading-youtube-videos-api.html#2-create-oauth-credentials","title":"2. Create OAuth Credentials","text":"<ol> <li> <p>Go to Credentials:</p> <ul> <li>In the left-hand menu, click Credentials.</li> <li>Click Create Credentials, and select OAuth 2.0 Client ID.</li> </ul> </li> <li> <p>Configure OAuth Consent Screen:</p> <ul> <li>You will be prompted to set up the OAuth consent screen.</li> <li>Select External for the user type, fill in basic app information (e.g., app name, support email), and Save.</li> </ul> </li> <li> <p>Create OAuth 2.0 Client ID:</p> <ul> <li>For Application type, select Desktop App.</li> <li>Name it (e.g., \"YouTube Data API Desktop Client\") and click Create.</li> <li>Download the <code>client_secret.json</code> file, which will be used in your Python script.</li> </ul> </li> </ol>"},{"location":"admin-guides/preservica/downloading-youtube-videos-api.html#3-set-up-your-python-environment","title":"3. Set Up Your Python Environment","text":"<ol> <li>Install the required Python libraries:       Run the following command to install the necessary dependencies:       <pre><code>pip install google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client yt-dlp\n</code></pre></li> </ol>"},{"location":"admin-guides/preservica/downloading-youtube-videos-api.html#4-run-the-python-script","title":"4. Run the Python Script","text":"<p>Use the following script to authenticate and list videos from the YouTube Brand Account:</p> <pre><code>import os\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\n# Set up the API scope for YouTube Data API v3\nscopes = [\"https://www.googleapis.com/auth/youtube.readonly\"]\n\n# Function to authenticate and get the YouTube service\ndef get_authenticated_service():\n    api_service_name = \"youtube\"\n    api_version = \"v3\"\n    client_secrets_file = \"client_secret.json\"  # Path to the client secret file\n\n    # Get credentials and create an API client\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(client_secrets_file, scopes)\n    credentials = flow.run_local_server(port=0)\n\n    youtube = googleapiclient.discovery.build(api_service_name, api_version, credentials=credentials)\n    return youtube\n\n# Function to list videos from the Brand Account\ndef list_brand_account_videos(youtube):\n    videos = []\n    request = youtube.search().list(\n        part=\"id,snippet\",\n        forMine=True,  # Fetches videos from the authenticated Brand Account\n        maxResults=50,\n        type='video'\n    )\n\n    while request is not None:\n        response = request.execute()\n\n        # Collect video IDs and titles\n        for item in response['items']:\n            if 'snippet' in item:\n                video_id = item['id']['videoId']\n                title = item['snippet']['title']\n                videos.append((title, f\"https://www.youtube.com/watch?v={video_id}\"))\n\n        request = youtube.search().list_next(request, response)\n\n    return videos\n\nif __name__ == \"__main__\":\n    # Get authenticated YouTube service\n    youtube_service = get_authenticated_service()\n\n    # List videos from the Brand Account\n    video_list = list_brand_account_videos(youtube_service)\n\n    # Print video URLs and titles\n    for title, url in video_list:\n        print(f\"Title: {title}, URL: {url}\")\n</code></pre>"},{"location":"admin-guides/preservica/downloading-youtube-videos-api.html#5-run-the-script","title":"5. Run the Script","text":"<ol> <li> <p>Place the <code>client_secret.json</code> file in the same directory as your script.</p> </li> <li> <p>Run the Python script:       <pre><code>python your_script.py\n</code></pre></p> </li> <li> <p>Sign in to Google: </p> <ul> <li>During execution, the script will open a browser window for OAuth authentication.</li> <li>Log in with the Google account that has manager access to the Brand Account.</li> <li>After selecting your Google account, ensure you switch to the Brand Account when prompted.</li> </ul> </li> <li> <p>View the List of Videos:</p> <ul> <li>The script will print the titles and URLs of all videos from the Brand Account, including public, private, and unlisted ones.</li> </ul> </li> </ol>"},{"location":"admin-guides/preservica/downloading-youtube-videos-api.html#6-optional-download-videos-using-yt-dlp","title":"6. (Optional) Download Videos Using yt-dlp","text":"<p>If you want to download the videos instead of just listing them, you can integrate yt-dlp into the script to handle the downloads, as outlined in the previous examples.</p> <p>Key Points: - OAuth Authentication: This process allows you to access the Brand Account's videos, even if they are private or unlisted, as long as you are authenticated as a manager - Cookies for Downloading: For downloading private/unlisted videos that require authentication, you may need to use cookies with <code>yt-dlp</code> (optional if you're just listing videos) - API Limits: The YouTube Data API has daily quotas (10,000 units/day) for free use, but listing videos for personal use should fall well within these limits</p> <p>Let me know if you need further clarification or assistance!</p>"},{"location":"admin-guides/preservica/downloading-youtube-videos-api.html#downloading-youtube-videos-via-api","title":"Downloading YouTube Videos via API","text":"<p>Note: This section is under development. Future additions will include: - Prerequisites section - API Setup section - Download Process section - Quality Settings section - Error Handling section - Best Practices section</p>"},{"location":"admin-guides/preservica/dublin-core.html","title":"Metadata Style Guide","text":""},{"location":"admin-guides/preservica/dublin-core.html#general","title":"General","text":"<p>Purpose: This page covers how ICAEW applies metadata in Preservica.</p>"},{"location":"admin-guides/preservica/dublin-core.html#asset-xip","title":"Asset XIP","text":"XIP Terms Description Examples entity.title General Use <ul><li>The name given to the resource.</li></ul>ICAEW Specific Use<ul><li>The title as found within the document.</li><li>Use sentence case (capitalize first word only).</li><li>Acronyms (e.g., OECD, IFRS, FRC, HMRC, UK, VAT) and proper nouns must always be all capitalised, even at the start or after a colon.</li><li>Separate title and subtitle with a colon (replace em-dashes, en-dashes, or hyphens with a colon).</li><li>Do not capitalize the first letter after a colon.</li><li>Do not use \"&amp;\"; use \"and\".</li><li>Use question marks where applicable, but do not end titles with full stops.</li><li>Order and format - title: subtitle, issue/volume, date.</li><li>Dates must be in a readable format like this: \"15th January 2024\".</li><li>Include if the document is revised or time-limited.</li><li>Use only standard ASCII characters (no smart quotes or special symbols).</li></ul> <ul><li>Economia, Issue 1</li><li>International consistency: global challenges initiative, providing direction</li><li>Preface to international reporting standards, TECH 02/02</li><li>OECD discussion draft on the application of tax treaties to state-owned entities: including sovereign wealth funds, TAXREP 4/10, 22nd January 2010</li><li>Audit firm governance: a project for the Financial Reporting Council, Ernst and Young LLP response, 3rd February 2009</li><li>Technical release: IFRS 9 implementation, TECH 01/24, 15th January 2024</li></ul> entity.description General Use <ul><li>An account of the content of the resource.</li></ul> ICAEW Specific Use<ul><li>Use only if an existing summary or description is present.</li><li>Briefly summarize the content if available (do not create new summaries manually).</li><li>Listing of contents may be used, with each item separated by a semicolon.</li><li>Always end the description with a full stop; question marks are also allowed.</li></ul> <ul><li>Technical guidance on implementing IFRS 9 for financial instruments.</li><li>This Audit and Assurance Faculty guidance sets out the steps auditors need to take to ascertain whether material uncertainty disclosures in relation to going concern in the financial statements are adequate, and how these disclosures will then impact the audit report. It supplements the guidance in the faculty's audit report guides.</li><li>This guide provides an overview of blockchain technology, including its key features; potential use cases; challenges to widespread adoption; a glossary of terms; and relevant resources. Includes a case study on Ripple and discusses both the strengths and limitations of blockchain for accountants.</li></ul>"},{"location":"admin-guides/preservica/dublin-core.html#folder-xip","title":"Folder XIP","text":"XIP Terms Description Examples entity.title <ul><li>Acronyms and proper nouns should be all capitalised appropriately.</li><li>Use sentence case.</li><li>Uses user-friendly names (i.e. prioritise access for folder level entity.title.).</li><li>Folders can be dated if they contain assets collected by date.</li></ul> <ul><li>Economia magazine</li><li>Economia, 2019</li></ul> entity.description General Use <ul><li>An account of the content of the resource.</li></ul> ICAEW Specific Use<ul><li>This field for folders is used to provide a general description of the collection within the folder.</li><li>Include a date range and relevant title history.</li><li>Do not use special punctuation.</li></ul> <ul><li>Economia is the journal of ICAEW (previously ICAEW updates were published in Accountancy). Economia ran from 2012 to 2019. Quarterly replaced Economia.</li></ul>"},{"location":"admin-guides/preservica/dublin-core.html#custom-icaew-schema","title":"Custom ICAEW Schema","text":"Custom ICAEW Schema Terms Description Examples icaew:ContentType <ul><li>We use a controlled vocabulary defined by the ICAEW's taxonomy.</li></ul> Must use one of the following controlled vocabulary terms (exact spelling and case): <ul><li>Annual report</li><li>Article</li><li>Committee papers</li><li>Database</li><li>eBook</li><li>eBook chapter</li><li>eLearning module</li><li>Event</li><li>Form</li><li>Helpsheets and support</li><li>Hub page</li><li>ICAEW consultation and response</li><li>Interview</li><li>Journal</li><li>Learning material</li><li>Legal precedent</li><li>Library book</li><li>Listing</li><li>Newsletter</li><li>No content type</li><li>Podcast</li><li>Press release</li><li>Promotional material</li><li>Regional news</li><li>Regulations</li><li>Report</li><li>Representation</li><li>Research guide</li><li>Speech or presentation</li><li>Synopsis</li><li>Technical release</li><li>Thought leadership report</li><li>Webinar</li><li>Website</li></ul>  If the content type cannot be determined from the document, use \"No content type\". icaew:InternalReference <ul><li>Format: YYYYMMDD-Document-Name</li><li>Use title case.</li><li>Allowed characters: letters (A-Z, a-z), numbers (0-9), hyphens (-). <li>Replace spaces with hyphens. No special characters. &amp; should be replaced with \"and\" if required.</li><li>Acronyms should be all capitals.</li><li>The YYYYMMDD is to be taken from within the document itself and not the document properties.</li><li>Use 00 to indicate absence of days/months/years. This field can begin with 00000000.</li><li>Include the document title and the faculty contributor, where applicable.</li><li>Include unique identifier. This will normally be an issue or reference number.</li> <ul><li>20120200-Economia-Issue-1</li><li>20101000-International-Consistency</li><li>20020117-TECH-02-02</li></ul> icaew:Notes <ul><li>At folder level this is used for internal note-taking purposes.</li><li>For folders used exclusively for deletion, this field explains reason for use and why it is to be deleted.</li><li>For collections, use this field to include information on who requested the ingest and who did the ingest.</li><li>This field is to be used to describe the providence of the work.</li></ul> <ul><li>EXAMPLE HERE?</li></ul>"},{"location":"admin-guides/preservica/dublin-core.html#asset-dublin-core","title":"Asset Dublin Core","text":"DC Terms Description Examples Title Use same guidance as in entity.title Creator General Use <ul><li>An entity primarily responsible for making the content.</li></ul> ICAEW Specific Use<ul><li>The creator is the author as credited in the document itself, not in its properties.</li><li>Multiple creators are allowed.</li><li>Preference for Creator is author FirstName LastName, internal faculty, and then institution (most often ICAEW).</li></ul> <ul><li>John Doe</li><li>Financial Services Faculty</li><li>Deloitte</li><li>ICAEW</li></ul> Subject General Use <ul><li>The topic of the content of the resource.</li></ul> ICAEW Specific Use<ul><li>Multiple dc:subject fields are allowed.</li><li>Subjects are selected from ICAEW\u2019s internal classification system (Semaphore).</li><li>Each document should be assigned 10 relevant subjects.</li></ul> Description Use same guidance as in entity.title Publisher General Use <ul><li>The entity responsible for making the resource available.</li></ul> ICAEW Specific Use<ul><li>Preference for external publishers, otherwise use ICAEW.</li><li>Check ICAEW Library catalogue for support.</li></ul> <ul><li>Progressive Content</li><li>ICAEW</li></ul> Contributor General Use <ul><li>An entity responsible for making contributions to the resource.</li></ul> ICAEW Specific Use<ul><li>This field is for external institutes.</li><li>Use the external institute\u2019s full title.</li></ul> <ul><li>Deloitte</li><li>The Pensions Regulator</li></ul> Date General Use <ul><li>A date associated with an event in the life cycle of the resource.</li></ul> ICAEW Specific Use<ul><li>Use the format: YYYY-MM-DD.</li><li>If the full date is unknown, use month and year (YYYY-MM) or just year (YYYY).</li></ul> <ul><li>2002-01-17</li><li>2012-02</li><li>2022</li></ul> Type General Use <ul><li>The nature or genre of the content of the resource.</li></ul> ICAEW Specific Use<ul><li>Contains DC recommended DCMI Type Vocabulary such as Text or Moving Image etc.</li><li>Use title case.</li></ul> <ul><li>Text</li><li>Moving Image</li><li>Audio</li><li>Interactive Resource</li></ul> Format General Use <ul><li>The physical or digital manifestation of the resource.</li></ul> ICAEW Specific Use<ul><li>Use the file's extension to indicate its format.</li><li>Use lowercase without preceding \".\".</li></ul> <ul><li>doc</li><li>xlsx</li><li>csv</li><li>pdf</li><li>mp4</li></ul> Identifier General Use <ul><li>An unambiguous reference to the resource within a given context.</li></ul> ICAEW Specific Use<ul><li>Examples of used Identifiers: ISBN, URLs, issue numbers, unique identifiers.</li><li>Multiple identifiers are allowed.</li></ul> <ul><li>https://www.icaew.com/technical/tax/tax-faculty/taxline</li><li>ISBN 978-1-78915-123-4</li><li>TECH 12/10 AAF</li><li>0_hbyzwx9t Source General Use <ul><li>A related resource from which the described resource is derived.</li></ul> ICAEW Specific Use<ul><li>Primarily used if an item has been digitised and has an original source since it is not a born-digital item.</li></ul> <ul><li>Digitised</li></ul> Language General Use <ul><li>A language of the intellectual content of the resource.</li></ul> ICAEW Specific Use<ul><li>Will normally be 'en' for English ICAEW publications.</li></ul> <ul><li>en</li><li>ar</li><li>cn</li></ul> Relation General Use <ul><li>A reference to a related resource.</li></ul> ICAEW Specific Use<ul><li>Populate with the name of the parent folder where the document resides.</li><li>Helps track document relationships within the archive hierarchy.</li><li>Documents linked into a separate parent folder/collection would have two Relation fields.</li></ul> <ul><li>Economia</li><li>Audit Monitoring Report</li><li>Faculty Serials</li></ul> Coverage General Use <ul><li>The spatial or temporal topic of the resource, spatial applicability of the resource, or jurisdiction under which the resource is relevant.</li></ul> ICAEW Specific Use<ul><li>ICAEW does not currently use this field, maybe in future.</li></ul> Rights General Use <ul><li>Information about rights held in and over the resource.</li></ul> ICAEW Specific Use<ul><li>ICAEW does not currently use this field, maybe in future. Rights information can be found within documents.</li></ul>"},{"location":"admin-guides/preservica/dublin-core.html#folder-dublin-core","title":"Folder Dublin Core","text":"DC Terms Description Examples Title General Use <ul><li>The name given to the resource.</li></ul> ICAEW Specific Use<ul><li>Use title case.</li><li>No special punctuation.</li><li>Use access-friendly titles and use commas to break between title and dates.</li></ul> <ul><li>Economia, 2019</li><li>Audit Monitoring Report</li><li>Faculty Serials</li></ul> Description General Use <ul><li>An account of the content of the resource.</li></ul> ICAEW Specific Use<ul><li>This field for folders is used to provide a general description of the collection within the folder.</li><li>Include a date range and relevant title history.</li><li>Do not use special punctuation.</li></ul> <ul><li>Economia is the journal of ICAEW (previously ICAEW updates were published in Accountancy). Economia ran from 2012 to 2019. Quarterly replaced Economia.</li></ul> Date General Use <ul><li>A date associated with an event in the life cycle of the resource.</li></ul> ICAEW Specific Use<ul><li>Date ranges should be included in the parent folder, using YYYY-MM-DD.</li><li>If the full date is unknown, month and year (YYYY-MM) or just year (YYYY) may be used.</li><li>Date ranges may be specified using ISO 8601 period of time specification in which start and end dates are separated by a '/' (slash) character.</li><li>Either the start or end date may be missing.</li></ul> TO BE DECIDED!(?) <ul><li>2012 - 2019</li><li>2012 / 2019</li></ul>"},{"location":"admin-guides/preservica/filename-conventions.html","title":"Filename Conventions","text":"<p>Purpose: This document outlines the filename conventions used for assets in the ICAEW Digital Archive.</p> <p>Note: This page is under development. Future additions will include: - Overview section - Naming Rules section - Special Cases section - Validation section - Examples section - Best Practices section</p>"},{"location":"admin-guides/preservica/filename-conventions.html#basic-format","title":"Basic Format","text":"<p>Assets in the digital archive follow the following format:</p> <ul> <li><code>YYYYMMDD-Title-of-Document(optional - Date|Vol)</code></li> </ul>"},{"location":"admin-guides/preservica/filename-conventions.html#guidelines","title":"Guidelines","text":"<p>Tip: When establishing a file name convention for digital preservation, it is crucial to prioritize clarity, consistency, and long-term accessibility.</p>"},{"location":"admin-guides/preservica/filename-conventions.html#key-principles","title":"Key Principles","text":"<ol> <li>Use descriptive and meaningful names</li> <li>Start the file name with a concise and informative description of the content</li> <li>Ensure it accurately reflects the file's contents</li> <li> <p>Example: <code>John-Doe-Interview-Transcript.pdf</code></p> </li> <li> <p>Include relevant metadata</p> </li> <li>Incorporate essential metadata into the file name, such as dates, creator names, and keywords</li> <li>This provides additional context and facilitates searchability</li> <li> <p>Example: <code>20230515-John-Doe-Interview-Transcript.pdf</code></p> </li> <li> <p>Employ version control</p> </li> <li>If multiple versions of the same file exist, include version numbers or dates to differentiate between them</li> <li> <p>Example: <code>20230515-John-Doe-Interview-Transcript-v1.pdf</code></p> </li> <li> <p>Avoid special characters and spaces</p> </li> <li>To ensure compatibility across various operating systems and software, use only alphanumeric characters, hyphens, and underscores</li> <li>Avoid spaces, as they can cause issues in certain systems</li> <li> <p>Example: <code>20230515-John-Doe-Interview-Transcript.pdf</code></p> </li> <li> <p>Maintain a consistent structure</p> </li> <li>Establish a consistent structure for your file names, allowing for easy sorting and organization</li> <li>Consider using a logical order such as date, followed by creator, and then content description</li> <li> <p>Example: <code>YYYYMMDD-Creator-ContentDescription.extension</code></p> </li> <li> <p>Use standardized file formats</p> </li> <li>Save files using widely supported and open file formats to enhance long-term accessibility and prevent obsolescence</li> <li>Consider using formats like PDF/A for documents, TIFF for images, and FLAC for audio</li> <li> <p>Example: <code>20230515-John-Doe-Interview-Transcript.pdf</code> (PDF/A format)</p> </li> <li> <p>Include leading zeros for sortable filenames</p> </li> <li>When using dates in your file names, include leading zeros to ensure proper sorting and avoid confusion</li> <li>Example: <code>20230515-John-Doe-Interview-Transcript.pdf</code></li> </ol> <p>Note: The above convention serves as an example, and you can adapt it to meet the specific requirements and needs of your digital preservation project.</p>"},{"location":"admin-guides/preservica/ingest-workflow.html","title":"Ingest Workflow","text":"<p>Purpose: This document outlines the ingest workflow for smaller ingests (not for bulk uploads utilising the API). This workflow happens after the appraisal workflow.</p>"},{"location":"admin-guides/preservica/ingest-workflow.html#tools-used","title":"Tools Used","text":"<p>This workflow utilises the following tools:</p> <ul> <li>Preservica New Gen UI - Drag and drop tool</li> <li><code>a_get_metadata.py</code></li> <li><code>semaphore-helper.py</code></li> <li><code>b_delete_metadata.py</code></li> <li><code>c_add_metadata_from_csv.py</code></li> <li><code>d_update_xip_from_csv.py</code></li> </ul>"},{"location":"admin-guides/preservica/ingest-workflow.html#step-1-create-an-ingest-bucket-in-preservica","title":"Step 1: Create an ingest bucket in Preservica","text":"<p>Location: <code>Root/ADMIN/Working Area/Ingest</code></p> <p>Bucket requirements: - Name folder in following style: <code>yyyymmdd-ingest-name-bucket</code> - Hierarchy of ingest should be decided during appraisal and represented within the bucket</p> <p>Important: Take note of the asset ID for the bucket. This is the Preservica reference number that we use for our custom tools. It will be necessary for later steps in the workflow.</p>"},{"location":"admin-guides/preservica/ingest-workflow.html#step-2-upload-resources-to-preservica","title":"Step 2: Upload resources to Preservica","text":"<p>Using the drag and drop feature, populate the bucket in accordance with the hierarchy decided on during appraisal.</p>"},{"location":"admin-guides/preservica/ingest-workflow.html#step-3-add-metadata","title":"Step 3: Add metadata","text":"<p>Purpose: The following workflow explains the process of creating a CSV that will then populate each uploaded resource with relevant metadata. Our style guide for metadata can be located here.</p>"},{"location":"admin-guides/preservica/ingest-workflow.html#3a1-a_get_metadatapy","title":"3a1: a_get_metadata.py","text":"<p>Command: <pre><code>python3 a_get_metadata.py [preservica reference number] [output.csv]\n</code></pre></p> <p>Purpose: This first step creates a custom CSV in accordance with the ICAEW Digital Archive metadata style guide. This CSV will be used to add metadata to ingested resources.</p> <p>Note: The CSV should have the following naming convention, remaining consistent with the bucket naming convention: <code>yyyymmdd-ingest-name-metadata.csv</code>.</p>"},{"location":"admin-guides/preservica/ingest-workflow.html#3a2-semaphore-helperpy","title":"3a2: semaphore-helper.py","text":"<p>Command: <pre><code>python3 semaphore-helper.py [preservica ref] [asset download location] [output location &gt; .csv]\n</code></pre></p> <p>Purpose: The semaphore-helper script is a multi-part tool that: - Downloads a copy of each required asset from Preservica - Uses the ICAEW taxonomy to classify up to ten subject terms for the Dublin Core subject fields</p> <p>Requirements: - Preservica reference number - Area for each asset to be downloaded to - Output preferably CSV</p> <p>Note: The CSV should have the following naming convention, remaining consistent with the bucket naming and metadata naming convention: <code>yyyymmdd-ingest-name-classification.csv</code>. These subject terms will be added to the custom CSV.</p>"},{"location":"admin-guides/preservica/ingest-workflow.html#3b-b_delete_metadatapy","title":"3b: b_delete_metadata.py","text":"<p>Command: <pre><code>python3 b_delete_metadata.py [preservica_ref]\n</code></pre></p> <p>Purpose: This script is run to ensure there is no doubling of metadata within Preservica.</p> <p>Warning: The <code>b_delete_metadata.py</code> permanently deletes metadata templates connected to the asset. It uses a Preservica reference number to locate which assets' metadata it will delete. Using a folder's Preservica reference number will delete each of its child files' metadata templates.</p> <p>Each time this script is used, a Y/N prompt occurs. After Y, the metadata is permanently deleted.</p>"},{"location":"admin-guides/preservica/ingest-workflow.html#3c-c_add_metadata_from_csvpy","title":"3c: c_add_metadata_from_csv.py","text":"<p>Command: <pre><code>python3 c_add_metadata_from_csv.py [csv_location]\n</code></pre></p> <p>Purpose: The <code>c_add_metadata_from_csv.py</code> tool adds the newly complete Dublin Core metadata CSV to the original assets within Preservica.</p> <p>Note: This tool uses the Preservica reference numbers in the CSV to locate where to add the newly completed Dublin Core metadata template.</p>"},{"location":"admin-guides/preservica/ingest-workflow.html#3d-d_update_xip_from_csvpy","title":"3d: d_update_xip_from_csv.py","text":"<p>Command: <pre><code>python3 d_update_xip_from_csv.py [csv_location]\n</code></pre></p> <p>Purpose: The <code>d_update_xip_from_csv.py</code> tool adds the newly complete XIP metadata CSV to the original assets within Preservica. XIP metadata includes the <code>entity.title</code> (file name) and <code>entity.description</code>. The asset security tag can also be changed using this tool, but it is recommended to do that within the Preservica GUI.</p> <p>Note: This tool uses the Preservica reference numbers in the CSV to locate where to add the newly completed XIP metadata template.</p>"},{"location":"admin-guides/preservica/ingest-workflow.html#suggested-checklist","title":"Suggested Checklist","text":"<ul> <li>[ ] Confirm folder hierarchy matches appraisal output  </li> <li>[ ] Ensure all relevant metadata fields are filled in CSV  </li> <li>[ ] Review fields for misspelling, or misalignment (i.e., fields filled out wrong)  </li> </ul>"},{"location":"admin-guides/preservica/post-ingest.html","title":"Post Ingest","text":"<p>Purpose: This document outlines the post-ingest workflow. This workflow is important for: - Ensuring the quality of metadata - Cleaning up the tools we use after ingest (for example, Preservica and Sharepoint) - Linking the resources to the broader ICAEW information ecosphere</p>"},{"location":"admin-guides/preservica/post-ingest.html#quality-assurance","title":"Quality Assurance","text":"<p>Purpose: Quality Assurance is currently a manual process. Metadata should be filled out carefully and precisely during ingest. Quality Assurance is a process which is done throughout the broader ingest workflow. We want to be as sure as possible that metadata is correct and accurate.</p> <p>During Post-Ingest, work through the following steps:</p> <ol> <li>Check for Completeness:</li> <li> <p>Ensure all required metadata fields are filled in (e.g., title, author, date, format, keywords)</p> </li> <li> <p>Validate Accuracy:</p> </li> <li> <p>Confirm dates, names, and other values are correct and consistent with the source material</p> </li> <li> <p>Enforce Consistency:</p> </li> <li>Ensure resources adhere to file naming conventions and Dublin Core style guide</li> </ol>"},{"location":"admin-guides/preservica/post-ingest.html#clean-preservica-and-sharepoint","title":"Clean Preservica and Sharepoint","text":"<p>Purpose: When you are sure of the quality of metadata, it is time to clean up after the ingest.</p> <p>Work through the following steps:</p> <ol> <li>During the ingest workflow, we have created a bucket in the ingest area of Preservica. After the ingest has been completed, the series needs to be moved to its correct spot within the Root structure of Preservica</li> <li>The remaining bucket needs to be removed and sent to the recycling bin in Preservica</li> <li>Add to the description field of the bucket that you have completed the ingest with a date stamp: <code>yyyymmdd</code></li> </ol> <p>Note: Often when ingesting materials, we have received the resources from the G:Drive within the VDI and transferred them out using Sharepoint. The previous steps can be used to clean up Sharepoint after ingest.</p>"},{"location":"admin-guides/preservica/post-ingest.html#add-to-catalogue","title":"Add to Catalogue","text":"<p>Note: Needs to be discussed with Niki, Janet and Kim.</p>"},{"location":"admin-guides/preservica/standard-ingest-workflow.html","title":"Standard Ingest Workflow","text":"<p>Purpose: This page outlines the critical steps of appraisal and pre-ingest processes that are followed before an asset is ingested into Preservica. These workflows ensure thorough evaluation, metadata assignment, and asset preparation to maintain integrity and compliance throughout the digital preservation lifecycle.</p> <p>Note: This page is under development. Future additions will include: - Prerequisites section - Required Tools section - Step-by-Step Guide section - Quality Checks section - Troubleshooting section - Best Practices section</p> <pre><code>flowchart LR\n    id1[Appraisal \\n\\n Brunnhilde \\n Exiftool] --&gt; id2\n    id2[Pre-ingest \\n\\n SIP creation via custom Python scripts \\n Fixity \\n Dublin Core metadata creation \\n File renaming] --&gt; id3[Post-ingest \\n\\n Miscellaneous tasks: \\n Update catalogue entries \\n Migration and checking quality]</code></pre>"},{"location":"admin-guides/preservica/standard-ingest-workflow.html#workflow-breakdown","title":"Workflow Breakdown:","text":"<ul> <li>Appraisal: Tools like Brunnhilde and Exiftool are used to evaluate and assess the digital assets for preservation.</li> <li>Pre-Ingest: Preparation steps include creating SIP (Submission Information Packages) through custom Python scripts, performing fixity checks, adding Dublin Core metadata, and ensuring proper file renaming.</li> <li>Post-Ingest: After ingestion, miscellaneous tasks such as updating catalogue entries, migration, and quality checking are performed to ensure asset integrity and accessibility.</li> </ul> <p>```</p>"},{"location":"admin-guides/preservica/yt-dlp-youtube-dl.html","title":"YouTube-dl / yt-dlp","text":""},{"location":"admin-guides/preservica/yt-dlp-youtube-dl.html#overview","title":"Overview","text":"<p>Purpose: Downloading videos from YouTube or other sites is done using youtube-dl.</p> <p>Note: From the youtube-dl GitHub repository: \"youtube-dl is a command-line program to download videos from YouTube.com and a few more sites. It requires the Python interpreter, version 2.6, 2.7, or 3.2+, and it is not platform specific. It should work on your Unix box, on Windows or on macOS. It is released to the public domain, which means you can modify it, redistribute it or use it however you like\".</p>"},{"location":"admin-guides/preservica/yt-dlp-youtube-dl.html#installation","title":"Installation","text":"<p>Tip: Installation is via pip (a virtual environment is recommended):</p> <p>Command: <pre><code>pip install youtube-dl\n</code></pre></p>"},{"location":"admin-guides/preservica/yt-dlp-youtube-dl.html#configuration","title":"Configuration","text":"<p>Purpose: The following configuration works best for ICAEW needs:</p> <p>Configuration features: - Outputs JSON metadata files which can be converted to .metadata files via Python script - <code>restrict-filenames</code> - \"Restrict[s] filenames to only ASCII characters, and avoid \"&amp;\" and spaces in filenames\" - Always saves the downloaded YouTube videos as mp4 format - Saves the files with <code>%(upload_date)s-%(title)s</code> format - The write-annotations, write-description, write-all-thumbnails, and sub parameters may not be necessary, but can be downloaded for completion sake</p> <p>Command: <pre><code>youtube-dl --write-annotations --write-description --write-info-json --write-all-thumbnails --all-subs --write-auto-sub --write-sub --restrict-filenames --format 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best' --output '%(upload_date)s-%(title)s' [YouTube Channel URL]\n</code></pre></p> <p>Note: The YouTube channel URL is provided as the last argument. The files will be downloaded to a folder where youtube-dl was run from. The most important files are the videos themselves and the json files which contain all metadata relating to the videos. The files will still need to be manually renamed, i.e., removal of illegal characters, such as outlined here: https://www.mtu.edu/umc/services/websites/writing/characters-avoid</p>"},{"location":"admin-guides/preservica/yt-dlp-youtube-dl.html#basic-commands","title":"Basic Commands","text":"<p>Reference: Parameters can be found at https://github.com/ytdl-org/youtube-dl#options.</p>"},{"location":"admin-guides/preservica/yt-dlp-youtube-dl.html#advanced-options","title":"Advanced Options","text":"<p>Note: Advanced options section to be added.</p>"},{"location":"admin-guides/preservica/yt-dlp-youtube-dl.html#best-practices","title":"Best Practices","text":"<p>Note: Best practices section to be added.</p>"},{"location":"admin-guides/preservica/yt-dlp-youtube-dl.html#appendices","title":"Appendices","text":""},{"location":"admin-guides/preservica/yt-dlp-youtube-dl.html#appendix-a-python-script","title":"Appendix A: Python Script","text":"<pre><code>\"\"\"\nReads JSON files from youtube-dl and produce .metadata files based on data \ncontained.\n\"\"\"\nimport json\nimport os\nfrom datetime import datetime\n\n# Directory where the .info.json files are\ndirectory = r'/media/kingston/work/youtube-dl/ICAEW-youtube'\nfor filename in os.listdir(directory):\n    # Filter by file extension\n    if os.path.splitext(filename)[1] == '.json':\n        # Open JSON file\n        with open(os.path.join(directory, filename)) as json_file:\n            data = json.load(json_file)\n        # Get data from JSON; strip whitespace and replace \\n and &amp; \n        title = data['title'].replace('&amp;','and').strip()\n        description = data['description'].replace('\\n',' ').replace('&amp;','and').strip()\n        uploader = data['uploader'].replace('&amp;','and').strip()\n        upload_date = data['upload_date']\n        # Convert to appropriate format\n        upload_date = datetime.strptime(data['upload_date'], '%Y%m%d').strftime('%Y-%m-%d')\n\n        # DC template\n        template_dc = (f\"\"\"&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n        &lt;oai_dc:dc xsi:schemaLocation=\"http://www.openarchives.org/OAI/2.0/oai_dc/ oai_dc.xsd\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:oai_dc=\"http://www.openarchives.org/OAI/2.0/oai_dc/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"&gt;\n            &lt;dc:title&gt;{title}&lt;/dc:title&gt;\n            &lt;dc:creator&gt;ICAEW&lt;/dc:creator&gt;\n            &lt;dc:subject&gt;&lt;/dc:subject&gt;\n            &lt;dc:description&gt;{description}&lt;/dc:description&gt;\n            &lt;dc:publisher&gt;{uploader} YouTube Channel&lt;/dc:publisher&gt;\n            &lt;dc:contributor&gt;&lt;/dc:contributor&gt;\n            &lt;dc:date&gt;{upload_date}&lt;/dc:date&gt;\n            &lt;dc:type&gt;MovingImage&lt;/dc:type&gt;\n            &lt;dc:identifier&gt;&lt;/dc:identifier&gt;\n            &lt;dc:source&gt;&lt;/dc:source&gt;\n            &lt;dc:language&gt;&lt;/dc:language&gt;\n            &lt;dc:relation&gt;&lt;/dc:relation&gt;\n            &lt;dc:coverage&gt;&lt;/dc:coverage&gt;\n            &lt;dc:rights&gt;&lt;/dc:rights&gt;\n        &lt;/oai_dc:dc&gt;\"\"\")\n\n        # Write template to .metadata files\n        with open(os.path.join(directory, filename).replace('.info.json', '.mp4.metadata'), 'w') as f:\n            f.write(template_dc)\n</code></pre>"},{"location":"admin-guides/preservica/yt-dlp-youtube-dl.html#appendix-b-bash-scripts-for-filtering-by-video-duration","title":"Appendix B: Bash Scripts for Filtering by Video Duration","text":"<p>Output duration of all videos in folder</p> <pre><code>    for f in *.mp4\n    do\n    dur=`ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 \"$f\"`\n    durint=${dur%.*}\n    echo -n \"$dur\\t\"\n    echo \"$f\"\n    done\n</code></pre> <p>Filter out videos less than 90 seconds long and move to new folder</p> <pre><code>    mkdir \"Sub 90 second videos\"\n    for f in *.mp4\n    do\n    dur=`ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 \"$f\"`\n    durint=${dur%.*}\n    if [ $durint -lt 90 ]\n    then\n        echo -n \"$dur\\t\"\n        echo \"$f\"\n        mv $f \"./Sub 90 second videos/$f\"\n    fi\n    done\n</code></pre>"},{"location":"admin-guides/web-archiving/appendix.html","title":"Appendix: Advanced Web Archiving Processes","text":"<p>Purpose: This appendix contains detailed documentation for specialized web archiving processes that are performed infrequently. These instructions are maintained for reference purposes.</p>"},{"location":"admin-guides/web-archiving/appendix.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>Internet Archive Wayback Machine</li> <li>Twitter Archiving</li> <li>Archive-It WARC Downloads</li> <li>WARC File Naming Conventions</li> </ul>"},{"location":"admin-guides/web-archiving/appendix.html#internet-archive-wayback-machine","title":"Internet Archive Wayback Machine","text":""},{"location":"admin-guides/web-archiving/appendix.html#overview","title":"Overview","text":"<p>Purpose: Process for downloading content from the Internet Archive Wayback Machine using the CDX Server API. Information and documentation can be found here: Wayback CDX Server API</p>"},{"location":"admin-guides/web-archiving/appendix.html#process-details","title":"Process Details","text":"<ol> <li> <p>Obtain Index</p> </li> <li> <p>The first step is to obtain an index of pages captured by the public IA crawls via the Internet Archive Wayback CDX Server API</p> </li> <li>Example API query: <code>http://web.archive.org/cdx/search/cdx?url=icaew.com&amp;output=json&amp;matchType=domain&amp;filter=mimetype:application.*</code></li> <li> <p>Note: Obtaining the index for a large site can produce a response with many lines and will often crash the browser when queried directly through the browser address bar</p> </li> <li> <p>Download Index</p> </li> <li> <p>To overcome browser limitations, use wget to request the index and save to a local json file:</p> </li> </ol> <pre><code>wget 'http://web.archive.org/cdx/search/cdx?url=icaew.com&amp;matchType=domain&amp;output=json' -O icaew-com-index.json\n</code></pre> <ul> <li> <p>This request matches all captured URLs from the icaew.com domain (including sub-domains) and outputs to icaew-com-index.json</p> </li> <li> <p>Process JSON</p> </li> <li>Use the ia_cdx_json_to_txt.py script to convert the local json file to a list of URLs</li> <li>The resulting URL list can be given to wget or another downloader/crawler</li> </ul>"},{"location":"admin-guides/web-archiving/appendix.html#twitter-archiving","title":"Twitter Archiving","text":""},{"location":"admin-guides/web-archiving/appendix.html#overview_1","title":"Overview","text":"<p>Purpose: Methods for archiving Twitter content, including full account histories and specific time periods.</p>"},{"location":"admin-guides/web-archiving/appendix.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Twitter account credentials (available on the Logins page)</li> <li> <p>Note: Regarding Tweet limits (from the Twitter Help Center):</p> </li> <li>Only the 800 most recent Tweets are displayed in the Tweets tab</li> <li>Only the latest 3,200 Tweets are displayed in the Tweets and replies tab</li> </ul>"},{"location":"admin-guides/web-archiving/appendix.html#available-tools","title":"Available Tools","text":""},{"location":"admin-guides/web-archiving/appendix.html#1-archivewebpage","title":"1. archiveweb.page","text":"<ul> <li>Description: The easiest way to capture a Twitter account</li> <li>Options: Chrome extension or Electron desktop application</li> <li>Features:</li> <li>Twitter-specific browser behaviours</li> <li>Advanced search capabilities</li> <li>Usage:</li> <li>For tweets older than 3,200, capture the advanced search results feed</li> <li>Example URL format: <code>https://twitter.com/search?q=(from%3AICAEW)%20until%3A2009-12-31%20since%3A2009-01-01&amp;src=typed_query&amp;f=live</code></li> </ul>"},{"location":"admin-guides/web-archiving/appendix.html#2-snscrape","title":"2. snscrape","text":"<ul> <li>Description: Tool for scraping all tweets (surpassing the 3,200 tweet limit)</li> <li>Output Options:</li> </ul> <pre><code># Get tweet URLs\nsnscrape twitter-user ICAEW &gt; ICAEW-tweet-URLs.txt\n\n# Get full tweet data\nsnscrape --jsonl twitter-user ICAEW &gt; ICAEW-tweets.json\n</code></pre> <ul> <li>Note: A snapshot of the ICAEW Twitter was taken using this tool on 24/11/2022 which includes the entire history of the @ICAEW account up until the snapshot. The .txt and .json files are currently being held in the admin section of Preservica. The ICAEW-tweet-URLs.txt was supplied to the crawler to ensure that all Tweets have been crawled by a web crawler.</li> </ul>"},{"location":"admin-guides/web-archiving/appendix.html#3-twarc","title":"3. twarc","text":"<ul> <li>Status: Tested, but not currently in use as does not meet our requirements</li> <li>Limitations: Only captures latest 3,200 tweets in JSON format</li> <li>Setup:</li> </ul> <pre><code># Create virtual environment (recommended)\npython3 -m venv venv\n. ./venv/bin/activate\n\n# Install twarc\npip install twarc\n\n# Configure API credentials\ntwarc2 configure\n</code></pre> <ul> <li>Note: A developer account needs to be setup. There is a development account setup using the same credentials as above. The login details are on Logins page.</li> </ul>"},{"location":"admin-guides/web-archiving/appendix.html#archive-it-warc-downloads","title":"Archive-It WARC Downloads","text":""},{"location":"admin-guides/web-archiving/appendix.html#overview_2","title":"Overview","text":"<p>Purpose: Process for downloading and managing WARC files from Archive-It.</p>"},{"location":"admin-guides/web-archiving/appendix.html#installation-and-configuration","title":"Installation and Configuration","text":"<ol> <li>Install Client</li> </ol> <pre><code>pip install py-wasapi-client\n</code></pre> <p>Tip: Best practice is to install in a virtual environment</p> <ol> <li>Configure Credentials    Create <code>~/.wasapi-client</code>:    <pre><code>[icaew]\nusername = craig.mccarthy@icaew.com\npassword = PASSWORD\n</code></pre></li> </ol>"},{"location":"admin-guides/web-archiving/appendix.html#common-commands","title":"Common Commands","text":"<pre><code># Get MD5 file manifest of all WARC files on Archive-It\nwasapi-client --profile icaew --manifest\n\n# Get number of WARC files available after 2022-01-01\nwasapi-client --profile icaew --collection 16306 --log /tmp/out.log --crawl-time-after 2022-01-01 --count\n\n# Download WARC files\nwasapi-client --profile icaew --collection 16306 --log /tmp/out.log --crawl-time-after 2022-01-01 -p 1\n</code></pre>"},{"location":"admin-guides/web-archiving/appendix.html#backup-process","title":"Backup Process","text":"<p>There are regular crawls on Archive-It that will need to be backed up onto Preservica - at the time of writing the Insights Daily Summary and the public ICAEW.com capture.</p> <ol> <li> <p>Get Checksums</p> </li> <li> <p>Run <code>get_asset_checksum_values.py</code> using the folder reference for \"WACZ/WARC Files\" (currently: f127c4de-f82f-4d8d-a7f7-d7552cdd4e97)</p> </li> <li>Run <code>wasapi-client --profile icaew --manifest</code> to get Archive-It checksums</li> <li> <p>Run <code>preservica-archive-it-checksum-cross-reference.py</code> to compare lists</p> </li> <li> <p>Download Missing Files    Use the following bash script:</p> </li> </ol> <pre><code>#!/bin/bash\n\n# Path to the text file to loop through\nfile_path=\"/home/digital-archivist/Documents/apps/py-wasapi-client/files-not-present-in-preservica.txt\"\n\n# Loop through each line in the file\nwhile read line; do\n\n# Extract the substring from the line (using sed command)\nsub_string=$(echo \"$line\" | sed 's/.*\\(substring\\).*/\\1/')\n\n# Use the substring in a shell command (replace with your own command)\n# Replace the above echo statement with your own shell command that uses the sub_string\necho \"Downloading $sub_string\"\nwasapi-client --profile icaew --filename $sub_string\n\ndone &lt; \"$file_path\"\n</code></pre>"},{"location":"admin-guides/web-archiving/appendix.html#ignored-collections","title":"Ignored Collections","text":"<p>The following collection IDs can be ignored:</p> <ul> <li>16126 (DELETEME-The Business Finance Guide - Local Brozzler)</li> <li>14885 (DELETEME-ION (Public) - OLD)</li> <li>14886 (DELETEME-ION (Excel Community) - OLD)</li> <li>14752 (DELETEME-Insights - Daily Summary (Broken 2))</li> <li>13348 (DELETEME-Insights - Daily Summary (Broken 1))</li> </ul>"},{"location":"admin-guides/web-archiving/appendix.html#warc-file-naming-conventions","title":"WARC File Naming Conventions","text":""},{"location":"admin-guides/web-archiving/appendix.html#overview_3","title":"Overview","text":"<p>Purpose: Before ingest into Preservica or uploading to Archive-It, WARC files must follow a specific naming convention to ensure proper identification and organization.</p>"},{"location":"admin-guides/web-archiving/appendix.html#naming-format","title":"Naming Format","text":"<p>WARC files should be named following this convention:</p> <pre><code>Prefix-Timestamp-Serial-Crawlhost.warc.gz\n</code></pre>"},{"location":"admin-guides/web-archiving/appendix.html#components","title":"Components","text":"<ul> <li>Prefix: An abbreviation reflecting the project or crawl that created the file</li> <li>Timestamp: 14-digit GMT timestamp indicating when the file was initially begun</li> <li>Serial: Increasing serial number within the process creating the files</li> <li>Crawlhost: Domain name or IP address of the machine creating the file</li> </ul>"},{"location":"admin-guides/web-archiving/appendix.html#example","title":"Example","text":"<p>For ICAEW use:</p> <pre><code>Business-finance-guide-20210304135532142-001-ICAEW.warc.gz\n</code></pre>"},{"location":"admin-guides/web-archiving/appendix.html#reference","title":"Reference","text":"<p>This naming convention follows the recommendations from the WARC 1.0 specification.</p>"},{"location":"admin-guides/web-archiving/browsertrix-behaviors.html","title":"Browsertrix-crawler custom behaviors","text":""},{"location":"admin-guides/web-archiving/browsertrix-behaviors.html#overview","title":"Overview","text":"<p>Purpose: This document provides information about creating and implementing custom behaviors for the Browsertrix web crawler. These behaviors are used to enhance the crawling process by automatically handling dynamic content, auto-playing videos, and implementing site-specific interactions.</p>"},{"location":"admin-guides/web-archiving/browsertrix-behaviors.html#prerequisites","title":"Prerequisites","text":""},{"location":"admin-guides/web-archiving/browsertrix-behaviors.html#template-documentation","title":"Template Documentation","text":"<ul> <li>Before writing custom behaviors, request an updated list of templates from the web team</li> <li>Record these templates in the SharePoint document: Sitecore-templates-for-testing.docx</li> <li>The URLs listed in this document form the basis for testing capture and playback functionality</li> </ul>"},{"location":"admin-guides/web-archiving/browsertrix-behaviors.html#testing-requirements","title":"Testing Requirements","text":"<ul> <li>Currently, there are approximately 6 JavaScript elements requiring custom behaviors</li> <li>These elements are documented in the Sitecore-templates-for-testing.docx</li> <li>A test crawl of the listed pages should be performed to verify capture and playback</li> </ul>"},{"location":"admin-guides/web-archiving/browsertrix-behaviors.html#development-process","title":"Development Process","text":""},{"location":"admin-guides/web-archiving/browsertrix-behaviors.html#1-learning-resources","title":"1. Learning Resources","text":"<ul> <li>Review the tutorial at browsertrix-behaviors GitHub page</li> <li>Understand the behavior development framework and requirements</li> </ul>"},{"location":"admin-guides/web-archiving/browsertrix-behaviors.html#2-testing-environment-setup","title":"2. Testing Environment Setup","text":"<p>Note: For development and testing, custom behaviors are loaded from a local folder. In production crawls, behaviors are loaded via URL (see Browsertrix-crawler for production configuration).</p> <p>Command for testing custom behaviors:</p> <pre><code>docker run -p 9037:9037 \\\n-v $PWD/crawls:/crawls \\\n-v $PWD/custom-behaviors/:/custom-behaviors/ \\\nwebrecorder/browsertrix-crawler:1.5.11 crawl \\\n--url [URLS] \\\n--customBehaviors /custom-behaviors/ \\\n--screencastPort 9037 \\\n--scopeType page \\\n--behaviors siteSpecific \\\n--generateWACZ\n</code></pre> <p>Key parameters: - <code>--url</code>: Accepts multiple URLs for testing - <code>--scopeType</code>: Set to \"page\" to crawl only specified pages - <code>--behaviors</code>: Set to \"siteSpecific\" to avoid confusion with default behaviors - <code>--generateWACZ</code>: Creates a WACZ file for easy testing</p>"},{"location":"admin-guides/web-archiving/browsertrix-behaviors.html#3-monitoring-and-verification","title":"3. Monitoring and Verification","text":"<ul> <li>Monitor the crawl at localhost:9037</li> <li>Verify custom behaviors are functioning as expected</li> <li>Test the resulting WACZ file using ReplayWeb.page</li> </ul>"},{"location":"admin-guides/web-archiving/browsertrix-behaviors.html#implementation-details","title":"Implementation Details","text":"<p>Note: The following file structure is for development and testing. In production, behaviors are loaded via URL from the GitHub repository (see Browsertrix-crawler).</p>"},{"location":"admin-guides/web-archiving/browsertrix-behaviors.html#file-structure","title":"File Structure","text":"<ul> <li>Custom behaviors are JavaScript functions in a .js file</li> <li>For testing: File must be located in a \"custom-behaviors\" folder</li> <li>Folder should be in the same directory as the docker command execution</li> <li>For production: Behaviors are loaded via URL from the digital-archiving-scripts repository</li> </ul>"},{"location":"admin-guides/web-archiving/browsertrix-behaviors.html#behavior-script","title":"Behavior Script","text":"<p>Note: The behavior script is available on the icaew-digital-archive GitHub page. The ICAEW behaviors file can be found at icaew-com-behaviors.js.</p>"},{"location":"admin-guides/web-archiving/browsertrix-behaviors.html#appendix","title":"Appendix","text":""},{"location":"admin-guides/web-archiving/browsertrix-behaviors.html#complete-behavior-script","title":"Complete Behavior Script","text":"<pre><code>class ICAEWBehaviors {\nstatic init() {\nreturn {\nstate: {},\n};\n}\n\nstatic get id() {\nreturn \"ICAEWBehaviors\";\n}\n\nstatic isMatch() {\nconst pathRegex = /^(https?:\\/\\/)?([\\w-]+\\.)*icaew\\.com(\\/.*)?$/;\nreturn window.location.href.match(pathRegex);\n}\n\nasync *run(ctx) {\nctx.log(\"In ICAEW Behavior!\");\n\n// Change navbar to pink - FOR TESTING\n// Select all elements matching the specified selector\n//    var navLinks = document.querySelectorAll('#u-nav .u-nav--links &gt; li &gt; a');\n\n// Iterate over each element and set its color to pink\n//    navLinks.forEach(function(link) {\n//        link.style.color = 'pink';\n//    });\n\n// Function to click a cookie consent button\nfunction clickCookieConsentButton() {\nconst buttonId = \"CybotCookiebotDialogBodyLevelButtonLevelOptinAllowAll\";\nconst element = document.getElementById(buttonId);\n\nif (element) {\nelement.click();\n} else {\nconsole.log(`Element with ID '${buttonId}' not found`);\n}\n}\n\n// Trigger the function\nclickCookieConsentButton();\n\n// Function to click buttons within a dynamic filter with a delay\nfunction clickButtonsInFilter(selector, delay = 500) {\nconst parentElement = document.querySelector(selector);\n\nif (parentElement) {\nconst buttons = parentElement.querySelectorAll(\"button\");\n\nfunction clickButtonAtIndex(index) {\nif (index &lt; buttons.length) {\nbuttons[index].click();\nsetTimeout(() =&gt; clickButtonAtIndex(index + 1), delay);\n}\n}\n\nclickButtonAtIndex(0);\n} else {\nconsole.log(`Parent element with selector '${selector}' not found`);\n}\n}\n\n// Trigger the function for \"dynamic filter\" buttons\nclickButtonsInFilter(\n\"div.c-filter.c-filter--dynamic &gt; div.c-filter__filters\"\n);\n\n// Function to click an element with a delay\nfunction clickElementWithDelay(selector, delay = 500) {\nconst element = document.querySelector(selector);\n\nif (element) {\nelement.click();\nsetTimeout(() =&gt; clickElementWithDelay(selector, delay), delay);\n} else {\nconsole.log(`Element with selector '${selector}' not found`);\n}\n}\n\n// Start clicking the \"pagination control\" with a delay\nclickElementWithDelay(\"div.c-navigation-pagination &gt; nav &gt; a.page.next\");\n\n// Function to click an element repeatedly until it is hidden\nfunction clickUntilHidden(selector, delay = 500) {\nconst interval = setInterval(() =&gt; {\nconst element = document.querySelector(selector);\n\nif (element &amp;&amp; getComputedStyle(element).display !== \"none\") {\nelement.click();\n} else {\nconsole.log(\n`Element with selector '${selector}' not found or hidden`\n);\nclearInterval(interval);\n}\n}, delay);\n}\n\n// Call the function for the \"more-link\"\nclickUntilHidden(\"div.more-link &gt; a\");\n\nyield ctx.Lib.getState(ctx, \"icaew-stat\");\n}\n}\n</code></pre>"},{"location":"admin-guides/web-archiving/browsertrix.html","title":"Browsertrix-crawler","text":""},{"location":"admin-guides/web-archiving/browsertrix.html#overview","title":"Overview","text":"<p>This document covers the process of crawling ICAEW.com using Browsertrix-crawler. While focused on ICAEW.com, the process should be applicable to other websites. The configuration files, Python scripts, and custom JavaScript files can be found in the digital-archiving-scripts repository.</p> <p>For comprehensive documentation on Browsertrix-crawler, including all configuration options and advanced features, refer to the official Browsertrix Crawler documentation.</p>"},{"location":"admin-guides/web-archiving/browsertrix.html#prerequisites","title":"Prerequisites","text":"<p>Before starting any crawl, ensure you have:</p> <ol> <li>Docker installed and running on your system</li> <li>The specific Browsertrix-crawler image version pulled:    <pre><code>docker pull webrecorder/browsertrix-crawler:1.5.11\n</code></pre> <p>Note: Using version 1.5.11 prevents compatibility issues that can occur with the latest versions. This ensures consistent behavior and avoids unexpected problems during crawls.</p> </li> <li>Required configuration files and scripts from the digital-archiving-scripts repository</li> <li>Sufficient disk space for the crawl</li> </ol>"},{"location":"admin-guides/web-archiving/browsertrix.html#crawl-types","title":"Crawl Types","text":"<p>This guide covers several types of crawls. Choose the appropriate type based on your requirements:</p> Crawl Type Use Case Authentication Template crawl Testing specific page templates Optional ICAEW.com (logged-in) Full site crawl with authentication Required ICAEW.com (public) Full site crawl without authentication Not required ICAEW.com patch crawl Targeted crawl for missing URLs Optional Simple domain crawl Straightforward domain-scope crawl Optional"},{"location":"admin-guides/web-archiving/browsertrix.html#template-crawl","title":"Template crawl","text":""},{"location":"admin-guides/web-archiving/browsertrix.html#1-creating-a-browser-profile","title":"1. Creating a Browser Profile","text":"<p>Purpose: This step creates a browser profile with your authentication cookies for the logged-in crawl. For detailed information, see the official documentation on Creating and Using Browser Profiles.</p> <p>Command: <pre><code>sudo docker run -p 6080:6080 -p 9223:9223 \\\n-v $PWD/crawls/profiles:/crawls/profiles/ \\\n-it webrecorder/browsertrix-crawler:1.5.11 \\\ncreate-login-profile --url \"https://my.icaew.com/security/Account/Login\"\n</code></pre></p> <p>Profile Creation Steps:</p> <ol> <li> <p>Open Chrome and navigate to http://localhost:9223/</p> </li> <li> <p>Configure browser settings:</p> </li> <li>Click \"Allow all cookies\"</li> <li> <p>Disable Brave shields</p> </li> <li> <p>Authentication:</p> </li> <li>Login with your credentials</li> <li> <p>Disable Brave shields again (if prompted)</p> </li> <li> <p>Clean up:</p> </li> <li> <p>Close the \"Discover the latest MyICAEW App\" banner</p> </li> <li> <p>Verification:</p> </li> <li>Navigate to a page with StreamAMG video player (e.g., https://www.icaew.com/for-current-aca-students/training-agreement/your-online-training-file-guide/online-training-file-videos)</li> <li> <p>Verify that the video element loads correctly</p> </li> <li> <p>Finalize:</p> </li> <li>Click \"Create Profile\"</li> </ol>"},{"location":"admin-guides/web-archiving/browsertrix.html#2-setting-up-and-starting-the-template-crawl","title":"2. Setting Up and Starting the Template Crawl","text":""},{"location":"admin-guides/web-archiving/browsertrix.html#prerequisites_1","title":"Prerequisites","text":"<p>Ensure you have the following files in your working directory:</p> <ul> <li><code>crawl-config.yaml</code> - Configuration file for the crawl</li> <li><code>seedFile.txt</code> - Contains URLs to crawl. This should be the URLs found in the template document.</li> </ul> <p>Note: The custom behavior script (<code>icaew-com-behaviors.js</code>) is loaded via URL from the digital-archiving-scripts repository, so no local copy is required. For information on creating custom behaviors, see the Browser Behaviors documentation.</p> <p>Required directory structure: <pre><code>$PWD/\n\u251c\u2500\u2500 seedFile.txt\n\u251c\u2500\u2500 crawl-config.yaml\n</code></pre></p>"},{"location":"admin-guides/web-archiving/browsertrix.html#configuration","title":"Configuration","text":"<p>Tip: For comprehensive configuration options, see the YAML Crawl Config documentation.</p> <p>Example <code>crawl-config.yaml</code> for template testing: <pre><code># For additional configuration options, see https://crawler.docs.browsertrix.com/user-guide/yaml-crawl-config/\n# A \"template test\" crawl; i.e. used for crawling all the types of templates found on ICAEW.com.\n# The crawl will only crawl pages defined in seedFile.txt and no others.\n\n# Basic setup\nprofile: /crawls/profiles/profile.tar.gz\nseedFile: /app/seedFile.txt\ncollection: template-test\nscreencastPort: 9037\ncustomBehaviors: https://raw.githubusercontent.com/icaew-digital-archive/digital-archiving-scripts/refs/heads/main/browsertrix-crawler%20files%20and%20scripts/icaew-com-behaviors.js\n\n# Additional options\nallowHashUrls: true\ncombineWARC: true\ngenerateWACZ: true\nworkers: 6\ntext:\n- to-warc\n- to-pages\nscreenshot: view\ndiskUtilization: 0\n\n# Crawl specific options\nscopeType: \"page-spa\"\n</code></pre></p>"},{"location":"admin-guides/web-archiving/browsertrix.html#starting-the-crawl","title":"Starting the Crawl","text":"<p>Command: <pre><code>sudo docker run -p 9037:9037 \\\n-v $PWD/crawls:/crawls \\\n-v $PWD/crawl-config.yaml:/app/crawl-config.yaml \\\n-v $PWD/seedFile.txt:/app/seedFile.txt \\\nwebrecorder/browsertrix-crawler:1.5.11 crawl \\\n--config /app/crawl-config.yaml\n</code></pre></p> <p>Monitor the crawl progress at: http://localhost:9037</p>"},{"location":"admin-guides/web-archiving/browsertrix.html#3-quality-assurance-and-verification","title":"3. Quality Assurance and Verification","text":"<p>After the crawl completes, perform the following verification steps:</p>"},{"location":"admin-guides/web-archiving/browsertrix.html#1-run-web_archive_validatorpy","title":"1. Run web_archive_validator.py","text":"<ul> <li>Use web_archive_validator.py to generate three CSV files:<ul> <li><code>matching_urls.csv</code> - Successfully crawled URLs</li> <li><code>missing_urls.csv</code> - URLs that failed to crawl</li> <li><code>non_200_urls.csv</code> - URLs that returned non-200 status codes</li> </ul> </li> <li>Investigate missing and non-200 URLs. If neccessary, create a new seedFile.txt with problematic URLs. Run a secondary crawl using the same configuration. It may be worth updating the Sitecore templates and examples document at this point (i.e. updating the URLs to the redirects and removing non-existent pages).</li> </ul>"},{"location":"admin-guides/web-archiving/browsertrix.html#2-qa-crawl","title":"2. QA Crawl","text":"<p>Tip: For more information on Quality Assurance crawling, see the official QA documentation.</p> <p>Run a QA crawl to verify the capture: <pre><code>sudo docker run -p 9037:9037 \\\n-v $PWD/crawls/:/crawls/ \\\n-it webrecorder/browsertrix-crawler:1.5.11 qa \\\n--qaSource /crawls/collections/template-test/template-test.wacz \\\n--collection template-qa \\\n--generateWACZ \\\n--profile /crawls/profiles/profile.tar.gz \\\n--customBehaviors https://raw.githubusercontent.com/icaew-digital-archive/digital-archiving-scripts/refs/heads/main/browsertrix-crawler%20files%20and%20scripts/icaew-com-behaviors.js \\\n--screencastPort 9037 \\\n--workers 6\n</code></pre></p> <p>Generate a QA report using extract_qa.py. Use this to futher inspect problematic URLs.</p>"},{"location":"admin-guides/web-archiving/browsertrix.html#3-manual-inspection","title":"3. Manual Inspection","text":"<ul> <li>Use replayweb.page to inspect the WACZ file</li> <li>Verify page rendering and functionality</li> <li>Go to the last captured page and ensure that it has remained \"logged-in\"</li> </ul>"},{"location":"admin-guides/web-archiving/browsertrix.html#full-icaewcom-crawl-logged-in","title":"Full ICAEW.com crawl (logged-in)","text":""},{"location":"admin-guides/web-archiving/browsertrix.html#1-creating-a-browser-profile_1","title":"1. Creating a Browser Profile","text":"<p>Purpose: This step creates a browser profile with your authentication cookies for the logged-in crawl. For detailed information, see the official documentation on Creating and Using Browser Profiles.</p> <p>Command: <pre><code>sudo docker run -p 6080:6080 -p 9223:9223 \\\n-v $PWD/crawls/profiles:/crawls/profiles/ \\\n-it webrecorder/browsertrix-crawler:1.5.11 \\\ncreate-login-profile --url \"https://my.icaew.com/security/Account/Login\"\n</code></pre></p> <p>Profile Creation Steps:</p> <ol> <li> <p>Open Chrome and navigate to http://localhost:9223/</p> </li> <li> <p>Configure browser settings:</p> </li> <li>Click \"Allow all cookies\"</li> <li> <p>Disable Brave shields</p> </li> <li> <p>Authentication:</p> </li> <li>Login with your credentials</li> <li> <p>Disable Brave shields again (if prompted)</p> </li> <li> <p>Clean up:</p> </li> <li> <p>Close the \"Discover the latest MyICAEW App\" banner</p> </li> <li> <p>Verification:</p> </li> <li>Navigate to a page with StreamAMG video player (e.g., https://www.icaew.com/for-current-aca-students/training-agreement/your-online-training-file-guide/online-training-file-videos)</li> <li> <p>Verify that the video element loads correctly</p> </li> <li> <p>Finalize:</p> </li> <li>Click \"Create Profile\"</li> </ol>"},{"location":"admin-guides/web-archiving/browsertrix.html#2-setting-up-and-starting-the-full-icaewcom-crawl","title":"2. Setting Up and Starting the Full ICAEW.com Crawl","text":""},{"location":"admin-guides/web-archiving/browsertrix.html#prerequisites_2","title":"Prerequisites","text":"<p>Required Files: Ensure you have the following files in your working directory:</p> <ul> <li><code>crawl-config.yaml</code> - Configuration file for the crawl</li> <li><code>seedFile.txt</code> - Contains URLs to crawl. This should be the URLs from the sitemap.</li> </ul> <p>Note: The custom behavior script (<code>icaew-com-behaviors.js</code>) is loaded via URL from the digital-archiving-scripts repository, so no local copy is required. For information on creating custom behaviors, see the Browser Behaviors documentation.</p> <p>Required directory structure: <pre><code>$PWD/\n\u251c\u2500\u2500 seedFile.txt\n\u251c\u2500\u2500 crawl-config.yaml\n</code></pre></p>"},{"location":"admin-guides/web-archiving/browsertrix.html#configuration_1","title":"Configuration","text":"<p>Tip: For comprehensive configuration options, see the YAML Crawl Config documentation.</p> <p>Example <code>crawl-config.yaml</code> for a full ICAEW.com crawl:</p> <pre><code># For additional configuration options, see https://crawler.docs.browsertrix.com/user-guide/yaml-crawl-config/\n# Example config for a full ICAEW.com crawl. Whether it is a logged-in session is defined within the crawl profile.\n# This crawl will read the seedFile.txt and also discover pages within the defined scope.\n\n# Basic setup\nprofile: /crawls/profiles/profile.tar.gz\nseedFile: /app/seedFile.txt\n# Collection name should be 'icaew-com-logged-in' or 'icaew-com-public'\ncollection: icaew-com-logged-in\nscreencastPort: 9037\ncustomBehaviors: https://raw.githubusercontent.com/icaew-digital-archive/digital-archiving-scripts/refs/heads/main/browsertrix-crawler%20files%20and%20scripts/icaew-com-behaviors.js\n\n# Additional options\nallowHashUrls: true\ncombineWARC: true\ngenerateWACZ: true\nworkers: 6\ntext:\n- to-warc\n- to-pages\nscreenshot: view\ndiskUtilization: 0\n\n# Crawl specific options\nscopeType: \"custom\"\ninclude:\n- ^(http(s)?:\\/\\/)?(www\\.)?(cdn\\.|regulation\\.)?icaew\\.com.*$ # scope in icaew.com, cd.icaew.com, and regulation.icaew.com\n- ^(http(s)?:\\/\\/)?(www\\.)?(train|volunteer)\\.icaew\\.com(\\/)?(blog.*)?$ # scope in parent and blog pages only\nexclude:\n- ^.*(l|L)(o|O)(g|G)(o|O)(f|F)(f|F).*$ # block logout URLs\n- ^(http(s)?:\\/\\/)?(www\\.)?icaew\\.com\\/search.*$ # block search pages (robots.txt)\n- ^(http(s)?:\\/\\/)?(www\\.)?.*\\/member(s|ship)\\/active-members.*$ # block active-members pages and media files\n- ^(http(s)?:\\/\\/)?(www\\.)?.*sprint-test-pages.*$ # block sprint-test-pages\n</code></pre>"},{"location":"admin-guides/web-archiving/browsertrix.html#starting-the-crawl_1","title":"Starting the Crawl","text":"<p>Command: <pre><code>sudo docker run -p 9037:9037 \\\n-v $PWD/crawls:/crawls \\\n-v $PWD/crawl-config.yaml:/app/crawl-config.yaml \\\n-v $PWD/seedFile.txt:/app/seedFile.txt \\\nwebrecorder/browsertrix-crawler:1.5.11 crawl \\\n--config /app/crawl-config.yaml\n</code></pre></p> <p>Monitor the crawl progress at: http://localhost:9037</p>"},{"location":"admin-guides/web-archiving/browsertrix.html#3-quality-assurance-and-verification_1","title":"3. Quality Assurance and Verification","text":"<p>After the crawl completes, perform the following verification steps:</p>"},{"location":"admin-guides/web-archiving/browsertrix.html#1-run-web_archive_validatorpy_1","title":"1. Run web_archive_validator.py","text":"<ul> <li>Use web_archive_validator.py to generate three CSV files:<ul> <li><code>matching_urls.csv</code> - Successfully crawled URLs</li> <li><code>missing_urls.csv</code> - URLs that failed to crawl</li> <li><code>non_200_urls.csv</code> - URLs that returned non-200 status codes</li> </ul> </li> <li>Investigate missing and non-200 URLs. If neccessary, create a new seedFile.txt with problematic URLs. Run a secondary crawl using the same configuration. The URLs in the <code>missing_urls.csv</code> file is obvious, but also look for 404/500 error URLs in <code>non_200_urls.csv</code> too. This secondary crawl should be done via crawl-config-icaew-com-one-hop.yaml, i.e. the ICAEW.com patch crawl {#icaewcom-patch-crawl}. This crawl config differs in the sense that it will only crawl the URLs from the seedFile.txt + 1 hop (to pickup things such as links to PDFs etc.) but will not discover/crawl the entire site like the primary crawl.</li> </ul>"},{"location":"admin-guides/web-archiving/browsertrix.html#2-final-packaging-of-warcs-into-wacz","title":"2. Final packaging of WARCs into WACZ","text":"<ul> <li> <p>Once you are happy with the QA and verification, use warc_processor.py to:</p> <ul> <li>Process a single or multiple WARC files into WACZ (Web Archive Collection Zipped) format</li> <li>Combine multiple WARC files into a single WARC file</li> <li>The command should look something like this:</li> </ul> </li> </ul> <pre><code>warc_processor.py --input \"/media/digital-archivist/Elements/20250426-ICAEW-COM/crawls/collect\nions/icaew-com-logged-in/archive\" --output \"/media/digital-archivist/Elements/20250426-ICAEW-COM/crawls/collections/icaew-com-logged-in/archive/20250426-ICAEW-com-logged-in.wacz\"\n</code></pre>"},{"location":"admin-guides/web-archiving/browsertrix.html#3-manual-inspection_1","title":"3. Manual Inspection","text":"<ul> <li>Use replayweb.page to inspect the WACZ file</li> <li>Verify page rendering and functionality</li> <li>Go to the last captured page and ensure that it has remained \"logged-in\"</li> </ul>"},{"location":"admin-guides/web-archiving/browsertrix.html#full-icaewcom-crawl-public","title":"Full ICAEW.com crawl (public)","text":"<p>Note: The process for a public crawl is similar to the logged-in crawl, with the main difference being the browser profile creation.</p>"},{"location":"admin-guides/web-archiving/browsertrix.html#creating-a-browser-profile","title":"Creating a Browser Profile","text":"<p>Purpose: This step creates a browser profile for the public crawl without authentication. For detailed information, see the official documentation on Creating and Using Browser Profiles.</p> <p>Command: <pre><code>sudo docker run -p 6080:6080 -p 9223:9223 \\\n-v $PWD/crawls/profiles:/crawls/profiles/ \\\n-it webrecorder/browsertrix-crawler:1.5.11 \\\ncreate-login-profile --url \"https://www.icaew.com/\"\n</code></pre></p> <p>Profile Creation Steps:</p> <ol> <li> <p>Open Chrome and navigate to http://localhost:9223/</p> </li> <li> <p>Configure browser settings:</p> </li> <li>Click \"Allow all cookies\"</li> <li> <p>Disable Brave shields</p> </li> <li> <p>Verification:</p> </li> <li>Navigate to a page with StreamAMG video player</li> <li> <p>Verify that the video element loads correctly</p> </li> <li> <p>Finalize:</p> </li> <li>Click \"Create Profile\"</li> </ol> <p>Note: The crawl-config is the same as the logged-in crawl, with the exception of the collection name.</p>"},{"location":"admin-guides/web-archiving/browsertrix.html#icaewcom-patch-crawl","title":"ICAEW.com patch crawl","text":"<p>Tip: This crawl config will only crawl the URLs from the seedFile.txt + 1 hop (to pickup things such as links to PDFs etc.) but will not discover/crawl the entire site like the full ICAEW.com crawls. For information on crawl scope configuration, see the Crawl Scope documentation.</p> <pre><code># For additional configuration options, see https://crawler.docs.browsertrix.com/user-guide/yaml-crawl-config/\n# Example config for a full ICAEW.com crawl. Whether it is a logged-in session is defined within the crawl profile.\n# This crawl will read the seedFile.txt and only crawl 1 hop from the seed URLs.\n\n# Basic setup\nprofile: /crawls/profiles/profile.tar.gz\nseedFile: /app/seedFile.txt\n# Collection name should be 'icaew-com-logged-in' or 'icaew-com-public'\ncollection: icaew-com-logged-in\nscreencastPort: 9037\ncustomBehaviors: https://raw.githubusercontent.com/icaew-digital-archive/digital-archiving-scripts/refs/heads/main/browsertrix-crawler%20files%20and%20scripts/icaew-com-behaviors.js\n\n# Additional options\nallowHashUrls: true\ncombineWARC: true\ngenerateWACZ: true\nworkers: 6\ntext:\n- to-warc\n- to-pages\nscreenshot: view\ndiskUtilization: 0\n\n# Crawl specific options\ndepth: 1  # Limit crawl to 1 hop from seed URLs\nscopeType: \"custom\"\ninclude:\n- ^(http(s)?:\\/\\/)?(www\\.)?(cdn\\.|regulation\\.)?icaew\\.com.*$ # scope in icaew.com, cd.icaew.com, and regulation.icaew.com\n- ^(http(s)?:\\/\\/)?(www\\.)?(train|volunteer)\\.icaew\\.com(\\/)?(blog.*)?$ # scope in parent and blog pages only\nexclude:\n- ^.*(l|L)(o|O)(g|G)(o|O)(f|F)(f|F).*$ # block logout URLs\n- ^(http(s)?:\\/\\/)?(www\\.)?icaew\\.com\\/search.*$ # block search pages (robots.txt)\n- ^(http(s)?:\\/\\/)?(www\\.)?.*\\/member(s|ship)\\/active-members.*$ # block active-members pages and media files\n- ^(http(s)?:\\/\\/)?(www\\.)?.*sprint-test-pages.*$ # block sprint-test-pages\n</code></pre>"},{"location":"admin-guides/web-archiving/browsertrix.html#simple-domain-crawl","title":"Simple domain crawl","text":"<p>Tip: This configuration example demonstrates a simple domain-scope crawl that will crawl all pages within the domain(s) listed in the seedFile.txt. This is useful for straightforward website crawls without complex scope requirements.</p> <pre><code># For additional configuration options, see https://crawler.docs.browsertrix.com/user-guide/yaml-crawl-config/\n\n# Basic setup\nprofile: /crawls/profiles/profile.tar.gz\nseedFile: /app/seedFile.txt\ncollection: cpia\nscreencastPort: 9037\ncustomBehaviors: https://raw.githubusercontent.com/icaew-digital-archive/digital-archiving-scripts/refs/heads/main/browsertrix-crawler%20files%20and%20scripts/icaew-com-behaviors.js\n\n# Additional options\nallowHashUrls: true\ncombineWARC: true\ngenerateWACZ: true\nworkers: 6\ntext:\n- to-warc\n- to-pages\nscreenshot: view\ndiskUtilization: 0\n\n# Crawl specific options\nscopeType: \"domain\" # This will crawl the pages/domains listed in the seedFile.txt\n</code></pre> <p>Important Notes: - With <code>scopeType: \"domain\"</code>, the crawler will automatically crawl all pages within the same domain as the seed URLs - Ensure your seedFile.txt includes the main homepage and any important entry points to ensure comprehensive coverage - For sites with multiple subdomains, consider using <code>scopeType: \"custom\"</code> with appropriate include patterns</p>"},{"location":"admin-guides/web-archiving/crawl-process-overview.html","title":"Web Archiving Process Overview","text":"<p>Purpose: This page outlines a full web capture of ICAEW.com crawl (logged-in / public) but the general process should apply to most other websites.</p>"},{"location":"admin-guides/web-archiving/crawl-process-overview.html#high-level-overview","title":"High-level overview","text":"<pre><code>flowchart LR\n    id1[Pre-crawl processes \\n\\n 1. Sitemap \\n 2. Template crawl \\n 3. Sub-domain discovery] --&gt; id2\n    id2[Crawl processes \\n\\n 1. Media library download \\n 2. wget crawl - logged in \\n 3. Browsertrix-crawler crawl - logged-in / public] --&gt; id3[Post-crawl processes \\n\\n 1. QA and verification \\n 2. Patch-crawls \\n 3. Ingest and metadata \\n 4. Documentation]</code></pre>"},{"location":"admin-guides/web-archiving/crawl-process-overview.html#preservica-folder-structure","title":"Preservica folder structure","text":"<p>Important: The following shows an example of a complete crawl within Preservica. Web crawls should follow this structure. All items should be closed with the exception of the .wacz file.</p> <pre><code>ICAEW.com, April 2025/\n\u251c\u2500\u2500 20250401-ICAEW-com-logged-in.wacz\n\u251c\u2500\u2500 Supplementary Materials/\n\u2502   \u251c\u2500\u2500 seedFile.txt [Mandatory if a seedFile.txt file was used]\n\u2502   \u251c\u2500\u2500 20250401-ICAEW-com-logged-in-wget.zip [Optional]\n\u2502   \u251c\u2500\u2500 20250401-ICAEW-com-media-library.zip [Optional]\n\u2502   \u251c\u2500\u2500 QA/\n\u2502   \u2502   \u251c\u2500\u2500 wacz_matching_urls_20250401_145826.csv [Optional]\n\u2502   \u2502   \u251c\u2500\u2500 wacz_missing_urls_20250401_145826.csv [Optional]\n\u2502   \u2502   \u251c\u2500\u2500 wacz_non_200_urls_20250401_145826.csv [Optional]\n\u2502   \u2502   \u251c\u2500\u2500 wget_matching_urls_20250401_145826.csv [Optional]\n\u2502   \u2502   \u251c\u2500\u2500 wget_missing_urls_20250401_145826.csv [Optional]\n\u2502   \u2502   \u251c\u2500\u2500 wget_non_200_urls_20250401_145826.csv [Optional]\n\u2502   \u2502   \u251c\u2500\u2500 notes.txt [Optional]\n</code></pre>"},{"location":"admin-guides/web-archiving/crawl-process-overview.html#process-overview","title":"Process Overview","text":""},{"location":"admin-guides/web-archiving/crawl-process-overview.html#pre-crawl-processes","title":"Pre-crawl processes","text":""},{"location":"admin-guides/web-archiving/crawl-process-overview.html#1-sitemap","title":"1) Sitemap","text":"<p>Purpose: Save a copy of the ICAEW sitemap for: - Input to the web crawlers - Post-crawl QA / validation</p> <p>Use sitemap_xml_to_txt_or_html.py with the following arguments:</p> <pre><code>python3 sitemap_xml_to_txt_or_html.py \\\n--to_file seedFile.txt \\\nhttps://www.icaew.com/sitemap_corporate.xml \\\n--exclude_strings \"sprint-test-pages\" \"active-members\" \\\n--deduplicate\n</code></pre>"},{"location":"admin-guides/web-archiving/crawl-process-overview.html#2-request-a-list-of-templates-and-do-a-template-crawl","title":"2) Request a list of templates and do a template crawl","text":"<p>Tip: Before running a full crawl, it's important to test how the crawler handles various templates and elements on the site - especially new ones. This helps identify potential issues with capture and playback.</p> <ul> <li> <p>Request any new additional templates be added to this document: Sitecore templates and examples </p> </li> <li> <p>Complete a template crawl</p> <ul> <li>This is covered on the Browsertrix-crawler page here.</li> <li>Optional: If problematic elements are identified during the template crawl, consider writing custom behaviors for the web crawler. This is outlined further here.</li> </ul> </li> </ul>"},{"location":"admin-guides/web-archiving/crawl-process-overview.html#3-sub-domain-discovery","title":"3) Sub-domain discovery","text":"<p>Optional: Use the crt-scraper.py script to discover new sub-domains that may not be included in a full ICAEW.com capture. This helps ensure comprehensive coverage of all ICAEW web properties.</p>"},{"location":"admin-guides/web-archiving/crawl-process-overview.html#crawl-processes","title":"Crawl processes","text":""},{"location":"admin-guides/web-archiving/crawl-process-overview.html#1-media-library-download","title":"1) Media library download","text":"<p>Prerequisites: - Access to the VDI - Sitecore login credentials (available on the Logins page)</p> <p>Steps:</p> <ol> <li>Log into the Sitecore backend</li> <li>Navigate to the \"Media Library\"</li> <li>Right-click the root level folder (\"Media Library\")</li> <li>Select \"Scripts\" and then \"Download\"</li> </ol> <p>Important Notes: - The download process may take several hours - The resulting zip file can be upwards of 10 GB - Only media stored directly in Sitecore will be downloaded. External media (e.g., Vimeo, StreamAMG) will not be included</p>"},{"location":"admin-guides/web-archiving/crawl-process-overview.html#2-wget-crawl","title":"2) wget crawl","text":"<p>Perform a wget-based crawl to create a backup copy of the website. This provides an alternative capture method and can be useful for verification purposes.</p> <ul> <li>Detailed instructions are available on the wget page.</li> </ul>"},{"location":"admin-guides/web-archiving/crawl-process-overview.html#3-browsertrix-crawler-logged-in-public-crawls","title":"3) Browsertrix-crawler - logged-in / public crawls","text":"<p>Execute the primary crawl using Browsertrix-crawler. This creates the main WACZ file that will be ingested into Preservica.</p> <ul> <li>For logged-in crawls: Requires browser profile creation with authentication cookies</li> <li>For public crawls: No authentication required</li> <li>Detailed instructions are available on the Browsertrix-crawler page.</li> </ul>"},{"location":"admin-guides/web-archiving/crawl-process-overview.html#post-crawl-processes","title":"Post-crawl processes","text":""},{"location":"admin-guides/web-archiving/crawl-process-overview.html#1-quality-assurance-and-verification","title":"1) Quality Assurance and Verification","text":"<p>Perform comprehensive QA checks to validate the crawl completeness and quality:</p> <ul> <li>Run validation scripts to compare crawled URLs against the sitemap</li> <li>Generate QA reports (matching URLs, missing URLs, non-200 status codes)</li> <li>Review and investigate any missing or problematic URLs</li> <li>Verify WACZ file integrity and playback functionality</li> </ul> <p>Detailed QA processes are covered on the wget page and the Browsertrix-crawler page.</p>"},{"location":"admin-guides/web-archiving/crawl-process-overview.html#2-patch-crawls","title":"2) Patch crawls","text":"<p>Conditional: If QA reveals missing URLs or problematic pages, perform targeted patch crawls to capture the missing content:</p> <ul> <li>Create a new seed file containing only the problematic URLs</li> <li>Run a patch crawl using a one-hop configuration (only crawls URLs in seed file plus one hop for linked resources)</li> <li>This prevents re-crawling the entire site while capturing missing content</li> <li>Detailed instructions are available in the Browsertrix-crawler patch crawl section</li> </ul>"},{"location":"admin-guides/web-archiving/crawl-process-overview.html#3-ingest-to-preservica","title":"3) Ingest to Preservica","text":"<p>Upload all crawl materials to Preservica following the established folder structure:</p> <ul> <li>Upload to Preservica using the AWS client</li> <li>Replicate the Preservica folder structure as outlined above</li> <li>Ensure all items are closed with the exception of the .wacz file</li> </ul> <p>Post-ingest verification: - Verify fixity values match using:   <pre><code>sha1sum [FILE]\n</code></pre> - Test playback of the WACZ file in Preservica's Portal to ensure proper rendering</p>"},{"location":"admin-guides/web-archiving/crawl-process-overview.html#4-write-basic-metadata-in-preservica","title":"4) Write basic metadata in Preservica","text":"<p>Add essential metadata to the crawl asset in Preservica to ensure proper discovery and access:</p> <ul> <li>Follow this example: ICAEW.com, April 2025</li> <li>Include capture date, crawl type (logged-in/public), and any relevant notes</li> </ul>"},{"location":"admin-guides/web-archiving/crawl-process-overview.html#5-documentation","title":"5) Documentation","text":"<p>Update relevant documentation to track the crawl completion:</p> <ul> <li>Update the ICAEW platform register document with the latest capture dates</li> <li>Document any issues encountered or special considerations in the crawl notes</li> </ul>"},{"location":"admin-guides/web-archiving/crawl-process-overview.html#6-upload-public-icaewcom-capture-to-archive-it","title":"6) Upload public ICAEW.com capture to Archive-It","text":"<p>TODO: Upload the public ICAEW.com capture to Archive-It for public access.</p>"},{"location":"admin-guides/web-archiving/monthly-web-captures.html","title":"Monthly Web Captures","text":""},{"location":"admin-guides/web-archiving/monthly-web-captures.html#overview","title":"Overview","text":"<p>Purpose: This document outlines the process for capturing three specific ICAEW webpages on a monthly basis using ArchiveWeb.page.</p>"},{"location":"admin-guides/web-archiving/monthly-web-captures.html#target-pages","title":"Target Pages","text":"<p>Schedule: The following three homepages are required to be captured on the second Monday of each month:</p> <ol> <li>Audit and Beyond</li> <li> <p>URL: https://www.icaew.com/technical/audit-and-assurance/faculty/audit-and-beyond</p> </li> <li> <p>By All Accounts</p> </li> <li> <p>URL: https://www.icaew.com/technical/corporate-reporting/corporate-reporting-faculty/by-all-accounts</p> </li> <li> <p>Taxline</p> </li> <li>URL: https://www.icaew.com/technical/tax/tax-faculty/taxline</li> </ol>"},{"location":"admin-guides/web-archiving/monthly-web-captures.html#tools","title":"Tools","text":"<p>Tool: For the monthly web captures, we use ArchiveWeb.page, a web archiving tool that allows for interactive capture of web content.</p>"},{"location":"admin-guides/web-archiving/monthly-web-captures.html#capture-process","title":"Capture Process","text":""},{"location":"admin-guides/web-archiving/monthly-web-captures.html#1-initial-setup","title":"1. Initial Setup","text":"<ol> <li>Open ArchiveWeb.page</li> <li>Create a new Archive</li> <li>Name the Archive following the WARC naming convention:    <pre><code>audit-and-beyond-19951012-000-ICAEW\n</code></pre> <p>Note: Additional GMT data will be added during Preservica ingestion</p> </li> </ol>"},{"location":"admin-guides/web-archiving/monthly-web-captures.html#2-crawl-execution","title":"2. Crawl Execution","text":"<ol> <li>Start the crawl from the three target homepages</li> <li>Begin in preview mode</li> <li>Log in to the ICAEW website using your work credentials</li> <li>After successful login, start the crawl</li> <li>During the crawl:</li> <li>Manually click through each tab on the webpage</li> <li>Ensure all interactive elements are captured</li> <li>Wait for the crawl to complete and all URLs to be collected</li> <li>Download the completed crawl</li> </ol>"},{"location":"admin-guides/web-archiving/monthly-web-captures.html#3-preservica-ingestion","title":"3. Preservica Ingestion","text":"<ol> <li>Rename the WACZ file to include GMT time:    <pre><code>audit-and-beyond-19951012111005-000-ICAEW\n</code></pre></li> <li>Ingest the files into the following Preservica folders:</li> <li>Audit and Beyond:       <pre><code>Admin/Private Repository/Web Captures/WACZ/WARC Files/audit-and-beyond-logged-in\n</code></pre></li> <li>By All Accounts:       <pre><code>Admin/Private Repository/Web Captures/WACZ/WARC Files/by-all-accounts-logged-in\n</code></pre></li> <li>Taxline:       <pre><code>Admin/Private Repository/Web Captures/WACZ/WARC Files/taxline-logged-in\n</code></pre></li> </ol>"},{"location":"admin-guides/web-archiving/monthly-web-captures.html#annual-series-creation","title":"Annual Series Creation","text":"<p>Note: At the end of each year, the monthly WACZ files for each webpage will be combined into an annual series, creating a comprehensive archive of the year's content. </p>"},{"location":"admin-guides/web-archiving/wget.html","title":"Wget Crawl","text":""},{"location":"admin-guides/web-archiving/wget.html#overview","title":"Overview","text":"<p>Purpose: This document outlines the process of using wget to create a secondary/back-up crawl of ICAEW.com and its subdomains.</p> <p>Wget is a free utility for non-interactive downloading of files from the web, supporting HTTP, HTTPS, and FTP protocols, as well as retrieval through HTTP proxies.</p> <p>Note: Wget crawls serve as a secondary/back-up capture method. The primary crawl should be performed using Browsertrix-crawler, which creates the main WACZ file for ingestion into Preservica. For the complete web archiving workflow, see the Web Archiving Process Overview.</p>"},{"location":"admin-guides/web-archiving/wget.html#prerequisites","title":"Prerequisites","text":"<p>Required: - Get cookies.txt LOCALLY browser extension - Access to ICAEW.com with appropriate credentials - Wget installed on your system - Sufficient disk space (crawls can generate several GB of data) - <code>seedFile.txt</code> containing URLs to crawl (typically a .txt version of the sitemap, one URL per line)</p>"},{"location":"admin-guides/web-archiving/wget.html#setup-process","title":"Setup Process","text":""},{"location":"admin-guides/web-archiving/wget.html#1-obtaining-cookies","title":"1. Obtaining Cookies","text":"<ol> <li>Install the Get cookies.txt LOCALLY browser extension</li> <li>Log in to ICAEW.com using a browser with the extension installed</li> <li>Export cookies using the extension</li> <li>Rename the exported file to <code>cookies.txt</code></li> <li>Move the file to your working directory</li> </ol>"},{"location":"admin-guides/web-archiving/wget.html#2-testing-authentication","title":"2. Testing Authentication","text":"<p>Tip: Before starting the full crawl, verify that the cookies file works by testing with a known restricted page:</p> <pre><code>wget --load-cookies cookies.txt \\\n--keep-session-cookies \\\n--page-requisites \\\n--adjust-extension \\\n--span-hosts \\\n--convert-links \\\n--restrict-file-names=windows \\\nhttps://www.icaew.com/technical/technology/excel-community/excel-community-articles/2023/dont-expect-too-much-from-ai-after-all-its-not-only-human\n</code></pre> <p>Verification: After the crawl has finished, navigate to the appropriate HTML file and ensure that it has remained logged in.</p>"},{"location":"admin-guides/web-archiving/wget.html#crawl-configurations","title":"Crawl Configurations","text":""},{"location":"admin-guides/web-archiving/wget.html#key-wget-flags-explained","title":"Key Wget Flags Explained","text":"<p>Before running crawls, it's helpful to understand some key flags:</p> <ul> <li><code>--recursive</code>: Follow links and crawl pages recursively (not used in non-recursive crawls)</li> <li><code>--page-requisites</code>: Download all files needed to display HTML pages (images, CSS, JavaScript)</li> <li><code>--span-hosts</code>: Allow downloading from different hosts (needed for CDN resources)</li> <li><code>--adjust-extension</code>: Add appropriate file extensions based on content type</li> <li><code>--convert-links</code>: Convert links in downloaded files to point to local files</li> <li><code>--restrict-file-names=windows</code>: Ensure filenames are Windows-compatible</li> <li><code>--no-parent</code>: Don't ascend to parent directories when using <code>--recursive</code></li> <li><code>--domains</code>: Restrict crawling to specified domains only</li> <li><code>--reject-regex</code>: Exclude URLs matching the regex pattern</li> </ul>"},{"location":"admin-guides/web-archiving/wget.html#directory-structure","title":"Directory Structure","text":"<p>Wget will create directories based on the domain structure. For example: - <code>www.icaew.com/</code> - Main site content - <code>regulation.icaew.com/</code> - Regulation subdomain content - Log files will be saved in the current working directory</p>"},{"location":"admin-guides/web-archiving/wget.html#crawl-configurations_1","title":"Crawl Configurations","text":"<p>The following crawls can be run simultaneously:</p>"},{"location":"admin-guides/web-archiving/wget.html#1-icaewcom","title":"1. ICAEW.com","text":"<p>Note: <code>seedFile.txt</code> should be a list of URLs, which will almost always be a .txt version of the sitemap.</p> <p>Important: This crawl configuration does not use <code>--recursive</code>, meaning it will only crawl the URLs explicitly listed in <code>seedFile.txt</code> and will not follow links. Media items (images, videos, etc.) are not included in the sitemap and therefore will not be crawled. If you need to crawl media items, use the Media Items Crawl configuration instead.</p> <pre><code>wget --load-cookies cookies.txt \\\n--keep-session-cookies \\\n--page-requisites \\\n--adjust-extension \\\n--convert-links \\\n--restrict-file-names=windows \\\n--regex-type pcre \\\n--reject-regex '((?i)(.*log(?:off|out).*))|((?i)(.*membership\\/active-members.*))' \\\n--random-wait \\\n--retry-connrefused \\\n--waitretry=10 \\\n--tries=3 \\\n--timeout=15 \\\n-i seedFile.txt 2&gt;&amp;1 | tee icaew-com-wget.log\n</code></pre>"},{"location":"admin-guides/web-archiving/wget.html#2-subdomains-regulation-train-volunteer","title":"2. Subdomains (Regulation, Train, Volunteer)","text":"<p>Note: This combined crawl covers regulation.icaew.com (full site), train.icaew.com (blog pages only), and volunteer.icaew.com (blog pages only). All three subdomains are crawled recursively from their seed URLs.</p> <pre><code>wget --load-cookies cookies.txt \\\n--keep-session-cookies \\\n--recursive \\\n--page-requisites \\\n--adjust-extension \\\n--span-hosts \\\n--convert-links \\\n--restrict-file-names=windows \\\n--domains regulation.icaew.com,train.icaew.com,volunteer.icaew.com \\\n--no-parent \\\n--regex-type pcre \\\n--reject-regex '((?i)(.*log(?:off|out).*))|(^(?!https:\\/\\/(train|volunteer)\\.icaew\\.com\\/?(blog\\/?.*|article\\/?.*|$)).+$)' \\\n--random-wait \\\n--retry-connrefused \\\n--waitretry=10 \\\n--tries=3 \\\n--timeout=15 \\\nregulation.icaew.com train.icaew.com volunteer.icaew.com 2&gt;&amp;1 | tee subdomains-wget.log\n</code></pre> <p>Note: The regex pattern restricts train.icaew.com and volunteer.icaew.com to blog/article pages only, while allowing full recursive crawling of regulation.icaew.com.</p>"},{"location":"admin-guides/web-archiving/wget.html#3-media-items-crawl-optional","title":"3. Media Items Crawl (Optional)","text":"<p>Note: If you need to crawl media items (images, videos, PDFs, etc.) that are not included in the sitemap, use this domain crawl configuration. This uses <code>seedFile.txt</code> as starting points and then recursively crawls from those URLs, following links to capture media library items.</p> <p>Warning: This crawl can take significantly longer and result in much larger capture sizes compared to the non-recursive seedFile.txt crawl, as it follows all links and downloads all media content.</p> <pre><code>wget --load-cookies cookies.txt \\\n--keep-session-cookies \\\n--recursive \\\n--page-requisites \\\n--adjust-extension \\\n--span-hosts \\\n--convert-links \\\n--restrict-file-names=windows \\\n--domains icaew.com \\\n--no-parent \\\n--regex-type pcre \\\n--reject-regex '((?i)(.*log(?:off|out).*))|((?i)(.*membership\\/active-members.*))' \\\n--random-wait \\\n--retry-connrefused \\\n--waitretry=10 \\\n--tries=3 \\\n--timeout=15 \\\n-i seedFile.txt 2&gt;&amp;1 | tee icaew-media-wget.log\n</code></pre> <p>Note: This configuration uses <code>seedFile.txt</code> as seed URLs and then recursively crawls from those pages, following links to capture media items that are not listed in the sitemap. This ensures you're crawling media linked from the pages you care about, rather than starting from just the homepage.</p>"},{"location":"admin-guides/web-archiving/wget.html#post-crawl-process","title":"Post-Crawl Process","text":"<ol> <li> <p>Log Analysis with wget_log_reader.py</p> <ul> <li>The script analyzes wget log files and compares them against the original URL list</li> <li> <p>Generates three CSV files with timestamps:</p> <ul> <li><code>matching_urls_[timestamp].csv</code>: URLs successfully crawled with 200 status</li> <li><code>missing_urls_[timestamp].csv</code>: URLs not found in the archives</li> <li><code>non_200_urls_[timestamp].csv</code>: URLs with non-200 status codes</li> </ul> <p>Usage: <pre><code>python wget_log_reader.py &lt;log_file_path&gt; &lt;url_file_path&gt;\n</code></pre></p> </li> </ul> </li> <li> <p>Verification Steps</p> </li> <li> <p>Review the generated CSV files</p> </li> <li>Investigate missing URLs and non-200 status codes</li> <li>Check redirect chains for any unexpected behavior</li> <li> <p>If needed, re-run the crawl but for the missing URLs only.</p> </li> <li> <p>Zipping and Ingest</p> </li> <li> <p>Add all of the folders to a parent folder called: <code>202XXXXX-ICAEW-com-logged-in-wget</code> and compress this into a zip file.</p> </li> <li>This file is to be ingested into Preservica along with the other elements of the capture.</li> </ol>"},{"location":"user-guides/overview.html","title":"Overview of the ICAEW Digital Archive","text":"<p>Purpose: The ICAEW Digital Archive consists of two main components: a digital preservation platform (Preservica) and web archive collections (hosted on both Preservica and Archive-It).</p>"},{"location":"user-guides/overview.html#preservica","title":"Preservica","text":"<p>Preservica is a cloud-based digital preservation platform used globally by organisations to manage and preserve digital content for the long term. It provides tools for ingest, storage, metadata management, preservation planning, and access, ensuring the ongoing availability, authenticity, and usability of digital content across changing hardware, software, and file formats.</p> <p>Key capabilities:</p> <ul> <li>Supports a wide range of content types: documents, images, audio, video, and web content</li> <li>Preserves assets throughout their lifecycle</li> <li>Provides robust search and retrieval functionality</li> <li>Ensures long-term accessibility despite technological changes</li> </ul>"},{"location":"user-guides/overview.html#how-icaew-uses-preservica","title":"How ICAEW Uses Preservica","text":"<p>ICAEW primarily uses Preservica for preserving:</p> <ul> <li>Documents: PDFs, DOCs, and other document formats</li> <li>Images: Photographs, graphics, and visual materials</li> <li>Audio: Podcasts and other audio recordings</li> <li>Video: Webinars, YouTube content, and video materials</li> <li>Web Collections: Some web archive collections are hosted in Preservica</li> </ul> <p>Key characteristics: - Content is curated and organised into a hierarchical folder structure - Detailed metadata supports search and retrieval - Structured organization makes content easier to navigate</p> <p>Note: Logged-in captures of ICAEW.com can be found in Preservica but not on Archive-It.</p>"},{"location":"user-guides/overview.html#web-archive-collections","title":"Web Archive Collections","text":"<p>A web archive is a collection of preserved web pages and digital content, enabling access to historical versions of websites. Web archiving involves capturing web pages and associated metadata (e.g., URLs, timestamps, file formats) using specialised software to ensure long-term availability and accessibility.</p> <p>Hosting platforms:</p> <ul> <li>Preservica - Some web collections</li> <li>Archive-It - Primary web archiving platform</li> </ul>"},{"location":"user-guides/overview.html#how-icaew-uses-web-archives","title":"How ICAEW Uses Web Archives","text":"<p>ICAEW primarily uses web archiving to periodically capture websites, most notably ICAEW.com.</p> <p>Key characteristics:</p> <ul> <li>Web archives are not curated\u2014they lack detailed metadata to aid search</li> <li>Content is captured as it appears on the web at specific points in time</li> <li>Navigation can be more challenging compared to curated Preservica content</li> <li>Provides historical snapshots of websites and online content</li> </ul>"},{"location":"user-guides/overview.html#quick-links","title":"Quick Links","text":"<ul> <li>Accessing Preservica</li> <li>Accessing Web Archives</li> <li>Why Digital Preservation and Web Archiving?</li> </ul>"},{"location":"user-guides/why-dp-web-archives.html","title":"Why Digital Preservation and Web Archiving?","text":"<p>Purpose: This guide explains the importance and benefits of digital preservation and web archiving for organizations, covering key advantages, use cases, legal requirements, and best practices.</p>"},{"location":"user-guides/why-dp-web-archives.html#what-is-digital-preservation","title":"What is Digital Preservation?","text":"<p>Digital preservation is the practice of maintaining the accessibility, usability, and integrity of digital information over time. It involves strategies and processes to ensure that digital content remains accessible and usable despite technological changes, media degradation, and other risks.</p> <p>Digital data faces numerous risks, including:</p> <ul> <li>Hardware and software obsolescence (learn more)</li> <li>Media degradation (learn more)</li> <li>Accidental or intentional data loss</li> <li>Cyberattacks</li> </ul> <p>As digital content becomes more pervasive, it is essential to safeguard this information for future generations. Without proper preservation efforts, significant historical, cultural, and scientific data may be lost forever.</p>"},{"location":"user-guides/why-dp-web-archives.html#what-is-web-archiving","title":"What is Web Archiving?","text":"<p>Web archiving is the process of capturing, preserving, and ensuring continued access to websites and online content over time. As the web evolves rapidly, web archiving has become an essential practice for preserving our digital heritage and meeting organizational needs.</p> <p>Key reasons for web archiving include:</p> <ul> <li>Preserving Cultural Heritage: Websites reflect and document cultural, societal, and political movements</li> <li>Supporting Historical Research: The web serves as a primary resource for understanding historical events and social changes</li> <li>Ensuring Legal and Regulatory Compliance: Many organisations are legally required to archive specific online content</li> <li>Protecting Business Continuity: Archiving corporate websites and social media ensures important digital assets are not lost</li> <li>Promoting Accountability and Transparency: Preserving public online content helps ensure transparency in digital public discourse</li> </ul>"},{"location":"user-guides/why-dp-web-archives.html#key-benefits","title":"Key Benefits","text":"<p>Digital preservation and web archiving offer numerous advantages for organizations:</p> <ul> <li>Long-term Access: Ensures continued access to digital content regardless of technological changes</li> <li>Risk Mitigation: Protects against data loss, corruption, and obsolescence</li> <li>Compliance: Helps meet legal and regulatory requirements for record retention</li> <li>Cost Efficiency: Reduces the need for repeated digitization and data recovery</li> <li>Knowledge Preservation: Maintains institutional memory and historical context</li> <li>Research Value: Supports academic and professional research with preserved digital resources</li> </ul>"},{"location":"user-guides/why-dp-web-archives.html#use-cases","title":"Use Cases","text":"<p>Digital preservation and web archiving serve various important purposes across different contexts:</p>"},{"location":"user-guides/why-dp-web-archives.html#organizational-use-cases","title":"Organizational Use Cases","text":"<ul> <li>Maintaining corporate records and communications</li> <li>Preserving digital publications and reports</li> <li>Archiving website content and social media presence</li> <li>Storing digital assets and multimedia content</li> <li>Documenting organizational history and decision-making processes</li> </ul>"},{"location":"user-guides/why-dp-web-archives.html#professional-use-cases","title":"Professional Use Cases","text":"<ul> <li>Supporting audit trails and compliance documentation</li> <li>Preserving professional standards and guidelines</li> <li>Maintaining historical financial records</li> <li>Archiving regulatory changes and updates</li> <li>Ensuring continuity of professional knowledge and practices</li> </ul>"},{"location":"user-guides/why-dp-web-archives.html#research-use-cases","title":"Research Use Cases","text":"<ul> <li>Supporting historical research and analysis</li> <li>Preserving digital cultural heritage</li> <li>Maintaining access to discontinued resources</li> <li>Enabling longitudinal studies and trend analysis</li> <li>Providing primary source material for academic research</li> </ul>"},{"location":"user-guides/why-dp-web-archives.html#legal-requirements","title":"Legal Requirements","text":"<p>Digital preservation is often mandated by various legal and regulatory frameworks. Many organisations are legally obligated to retain certain data for specified periods, and preservation ensures that this information remains accurate and accessible.</p> <p>Key regulatory considerations include:</p> <ul> <li>Data Protection Regulations: Requirements for secure storage and access control (e.g., GDPR, Data Protection Act)</li> <li>Industry Standards: Professional guidelines for record retention in specific sectors</li> <li>Compliance Mandates: Legal obligations for maintaining certain records for defined periods</li> <li>Intellectual Property: Protection of digital assets and copyright materials</li> <li>Audit Requirements: Maintaining accessible records for auditing purposes</li> <li>Freedom of Information: Ensuring public access to archived government and public sector records</li> </ul>"},{"location":"user-guides/why-dp-web-archives.html#additional-resources","title":"Additional Resources","text":""},{"location":"user-guides/why-dp-web-archives.html#external-resources","title":"External Resources","text":"<ul> <li>Digital Preservation Coalition - Leading organization for digital preservation best practices</li> <li>International Internet Preservation Consortium - Global organization for web archiving</li> <li>National Archives Digital Preservation - UK government guidance on digital preservation</li> <li>ISO 14721:2012 (OAIS) - Open Archival Information System reference model</li> </ul>"},{"location":"user-guides/preservica/accessing-preservica.html","title":"Accessing and Searching Preservica","text":""},{"location":"user-guides/preservica/accessing-preservica.html#access","title":"Access","text":""},{"location":"user-guides/preservica/accessing-preservica.html#two-methods-of-access","title":"Two methods of access","text":"<p>Purpose: There are two methods of accessing / searching Preservica:</p> <ul> <li>Preservica user access portal: https://icaew.access.preservica.com/</li> <li>Links from the library catalogue</li> </ul> <p>Example: An example from the library catalogue - http://libcat.icaew.com/uhtbin/cgisirsi/x/0/0/57/5/0?searchdata1=102535%7BCKEY%7D&amp;searchfield1=GENERAL%5ESUBJECT%5EGENERAL%5E%5E&amp;user_id=WEBSERVER; the Preservica link links directly to the document from the Electronic Access (856) field.</p> <p>Note: Currently the catalogue records contain far more detailed notes/metadata than the Preservica entries.</p>"},{"location":"user-guides/preservica/accessing-preservica.html#always-ensure-that-you-are-logged-in","title":"Always ensure that you are logged in","text":"<p>Important: Before proceeding with either access method, you should ensure that you are logged in (unless you are absolutely certain that you are looking for an item in a public collection). An overview of the restricted / public collections is available here.</p> <p>Note: If you are not logged in, you will either not see the restricted content at all or will be met with a message \"You must be logged in to the ICAEW Digital Archive to view this content\".</p> <p>Access Request: If you do not have a Preservica account, please request one from the Digital Archive Manager / Digital Archivist.</p>"},{"location":"user-guides/preservica/accessing-preservica.html#search","title":"Search","text":""},{"location":"user-guides/preservica/accessing-preservica.html#searching-via-the-preservica-web-portal","title":"Searching via the Preservica web portal","text":""},{"location":"user-guides/preservica/accessing-preservica.html#standard-search","title":"Standard search","text":"<p>Note: Searching via the library catalogue or libcat is not covered in this guide.</p> <p>Discovery can be made in two ways in Preservica: - Navigating directly through the folder hierarchies - Using the search functionality, including full-text search, filters, and facets</p> <p>Tip: The search bar in the web portal is by default a full-text search. Therefore, in most cases it will be sensible to use quotation marks to search for phrases, unless your search term is very specific. For example, searching making tax digital or \"making tax digital\" will yield ~1500 and ~640 results respectively. It is more likely that the second approach will return more relevant results.</p> <p>You are able to further refine your search by clicking on the facets that appear in the left \"REFINE YOUR SELECTION\" column and by clicking on the \"+FILTER\" button to bring down a menu where you can apply filters.</p> <p>Currently there are four filters that you can apply:</p> <ul> <li>Title</li> <li>Date</li> <li>ICAEW Department</li> <li>File name</li> </ul> <p></p>"},{"location":"user-guides/preservica/accessing-preservica.html#searching-via-the-filters-only","title":"Searching via the filters only","text":"<p>Tip: Sometimes it may be useful to search Preservica using only a filter. The most useful filter will often be the Title field.</p> <p>Note: The following process is not particularly intuitive but it works.</p> <p>The first step is to click the search icon in the navigation bar. This in effect searches for an empty string and will return all of the content in Preservica.</p> <p></p> <p>After this you should see that Preservica returned a large number of results (in the example below ~21,000, this includes a number of items only available in the Admin area viewable only by the Digital Archive Manager / Digital Archivist).</p> <p></p> <p>From this screen you can then apply a filter to search  via the Title field only. </p> <p>In the following example, you can see that Preservica contains 3 documents with \"artificial intelligence\" in the Title metadata.</p> <p></p> <p>Note: Currently there are a lot of Preservica documents missing the Title metadata. However, in the future the Title metadata will be added for all Preservica content, making this search method more usable.</p>"},{"location":"user-guides/preservica/preservica-overview.html","title":"Overview","text":"<p>Purpose: Preservica houses both public and restricted collections.</p> <p>Collection Types: - Public collections - Contain assets that are fully accessible to the public - Restricted collections - Contain assets that require logging in to view</p> <p>Access: To access the restricted collections, please ensure you are logged in at the access portal: https://icaew.access.preservica.com/.</p> <p>Note: A username and password can be obtained upon request from the Digital Archive Manager or Digital Archivist.</p>"},{"location":"user-guides/web-archiving/accessing-the-web-archives.html","title":"Accessing and Searching Web Archives","text":""},{"location":"user-guides/web-archiving/accessing-the-web-archives.html#overview-of-captures","title":"Overview of Captures","text":"<p>Purpose: This guide primarily focuses on full ICAEW.com captures, though the same principles apply to other collections.</p> <p>Web collections are classified as either public or restricted: - Public captures - Full website crawls from a public perspective, without access to content behind login credentials - Restricted captures - Full website crawls from a logged-in perspective, capturing content accessible only to authenticated users</p> <p>Note: For a comprehensive understanding of ICAEW.com captures, refer to the Web Crawl Process Overview.</p>"},{"location":"user-guides/web-archiving/accessing-the-web-archives.html#restricted-captures-on-preservica","title":"Restricted Captures on Preservica","text":""},{"location":"user-guides/web-archiving/accessing-the-web-archives.html#accessing-archived-pages","title":"Accessing Archived Pages","text":"<p>Restricted web captures are hosted on Preservica under the top-level folder: Web Archives.</p> <p>Important: You must be logged into Preservica to see the restricted collections. Without doing this, you will only see the public collections.</p> <p>To explore a collection:</p> <ol> <li>Navigate to the desired folder in the portal.</li> <li>Viewing the capture is made possible by the in-built PyWb-based web archive player.</li> </ol> <p>Example of a restricted web archive containing 16,982 pages:  </p> <p></p> <p>Tip: Clicking on a URL will open the archived webpage. A good tip is to make use of the \"Full Screen\" functionality:</p> <p></p>"},{"location":"user-guides/web-archiving/accessing-the-web-archives.html#full-text-search-in-preservica","title":"Full-Text Search in Preservica","text":"<p>Purpose: Preservica supports full-text search for certain collections, provided the text has been indexed. Use this feature to locate specific content by keyword or phrase.</p> <p>Example of a full-text search result:  </p> <p></p>"},{"location":"user-guides/web-archiving/accessing-the-web-archives.html#public-captures-on-archive-it","title":"Public Captures on Archive-It","text":"<p>Note: Public web collections on Archive-It are identified by a collection ID. For ICAEW.com public captures, the collection ID is 16306. </p>"},{"location":"user-guides/web-archiving/accessing-the-web-archives.html#accessing-archived-pages_1","title":"Accessing Archived Pages","text":"<p>To check if a URL has been captured:</p> <ol> <li>Prepend the target URL with: <code>https://wayback.archive-it.org/16306/*/</code></li> <li>Example:    Target URL: <code>https://www.icaew.com/technical/Technology/Artificial-intelligence</code>    Archive-It URL: https://wayback.archive-it.org/16306/*/https://www.icaew.com/technical/Technology/Artificial-intelligence</li> </ol> <p>The resulting Archive-It page provides: - A timeline of captures. - Dates when the page was archived. - Links to archived versions.</p> <p>If the page is missing, a \"not found\" message will be displayed.</p>"},{"location":"user-guides/web-archiving/accessing-the-web-archives.html#full-text-search-in-archive-it","title":"Full-Text Search in Archive-It","text":"<p>Note: Archive-It supports full-text search within public collections, unlike the Internet Archive.</p> <ol> <li>Visit the collection search page: https://archive-it.org/collections/16306</li> <li> <p>Click Search Page Text:  </p> <p></p> </li> <li> <p>Set Results per page to Unlimited:  </p> <p></p> </li> <li> <p>Enter your search query using these options:</p> </li> <li>Contains all of: Includes all specified terms.</li> <li>Exact phrase: Searches for an exact match.</li> <li>Not containing: Excludes specific terms.</li> <li>Optionally, filter results by date range.</li> <li>Click Advanced Search to start the search.</li> </ol> <p>For detailed instructions, refer to the Archive-It Search Guide.</p>"},{"location":"user-guides/web-archiving/accessing-the-web-archives.html#captures-predating-the-icaew-digital-archive","title":"Captures Predating the ICAEW Digital Archive","text":"<p>Note: For captures preceding the ICAEW digital archive, use the Internet Archive's Wayback Machine.</p>"},{"location":"user-guides/web-archiving/accessing-the-web-archives.html#accessing-older-captures","title":"Accessing Older Captures","text":"<p>To search for older versions of a URL:</p> <ol> <li>Prepend the URL with: <code>https://web.archive.org/web/*/</code></li> <li>Example:    Target URL: <code>https://www.icaew.com/technical/Technology/Artificial-intelligence</code>    Internet Archive URL: https://web.archive.org/web/*/https://www.icaew.com/technical/Technology/Artificial-intelligence</li> </ol>"},{"location":"user-guides/web-archiving/web-archive-overview.html","title":"Overview","text":"<p>Purpose: Web archives are organised into public and restricted collections:</p> <p>Collection Types: - Public collections - Contain web pages that are fully accessible to the general public - Restricted collections - Include web pages captured with a \"logged-in\" view, which may contain premium or staff-only content</p>"},{"location":"user-guides/web-archiving/web-archive-overview.html#public-web-archive-collections","title":"Public Web Archive Collections","text":"<p>Public web archive collections can be accessed via:</p> <ul> <li>Preservica \u2013 Takes you to the \"Web Archives\" folder in Preservica, which contains both public and restricted collections. Ensure you are logged in to access restricted collections.</li> <li>Archive-It \u2013 Directs you to the collections page for public web archives.</li> </ul>"},{"location":"user-guides/web-archiving/web-archive-overview.html#restricted-web-archive-collections","title":"Restricted Web Archive Collections","text":"<p>Restricted web archive collections can be accessed via (except for 2020 captures listed below):</p> <ul> <li>Preservica \u2013 Access both public and restricted web archive collections. Log in to Preservica to view restricted collections.</li> </ul>"},{"location":"user-guides/web-archiving/web-archive-overview.html#restricted-web-archive-collections-prior-to-2021","title":"Restricted Web Archive Collections Prior to 2021","text":"<p>Note: The following restricted web archive collections were hosted on Archive-It in 2020. Newer restricted collections are available exclusively on Preservica.</p> Collection Access URL Notes ICAEW.com (Restricted) Link Includes only 2020 captures. Newer captures are stored offline and available upon request. ION (Excel Community) Link Captured November 2020 ION (Healthcare Community) Link Captured November 2020 ION (Student Community) Link Captured November 2020"}]}